{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Modal Model Training - Independent Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import time\n",
    "import datetime\n",
    "import re\n",
    "import emoji\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import BertTokenizer, BertModel, BertConfig, AdamW\n",
    "from transformers import DistilBertTokenizer, DistilBertModel, DistilBertConfig\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler, RandomSampler, SequentialSampler, ConcatDataset\n",
    "from sklearn.model_selection import KFold\n",
    "import skimage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['Smears', 'Loaded Language', 'Name calling/Labeling', 'Glittering generalities (Virtue)',\n",
    "               'Appeal to (Strong) Emotions', 'Appeal to fear/prejudice', 'Transfer', 'Doubt',\n",
    "               'Exaggeration/Minimisation', 'Whataboutism', 'Slogans', 'Flag-waving',\n",
    "               \"Misrepresentation of Someone's Position (Straw Man)\", 'Causal Oversimplification',\n",
    "               'Thought-terminating clich√©', 'Black-and-white Fallacy/Dictatorship', 'Appeal to authority',\n",
    "               'Reductio ad hitlerum', 'Repetition', 'Obfuscation, Intentional vagueness, Confusion',\n",
    "               'Presenting Irrelevant Data (Red Herring)', 'Bandwagon']\n",
    "\n",
    "### Create Class Binarizer\n",
    "one_hot = MultiLabelBinarizer()\n",
    "one_hot.fit([classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CustomLoader import MultiModalLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = MultiModalLoader(json_file = 'MultiModal_training_data.json', root_dir = 'Images',\n",
    "                           transform = transforms.Compose([\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Resize(size = (224,224)),\n",
    "                               transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)) ### Pixel range [-1,1]\n",
    "                           ]))\n",
    "\n",
    "testing_data = MultiModalLoader(json_file = 'MultiModal_testing_data.json', root_dir = 'Images',\n",
    "                           transform = transforms.Compose([\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Resize(size = (224,224)),\n",
    "                               transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)) ### Pixel range [-1,1]\n",
    "                           ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset = training_data, batch_size = 25, shuffle = True)\n",
    "test_loader = DataLoader(dataset = testing_data, batch_size = 25, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "# \n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_lower_case = True\n",
    "model_type = 'distilbert'\n",
    "model_version = 'distilbert-base-uncased'\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_version, do_lower_case=do_lower_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define DistilBert Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistilBertClass(nn.Module): ### INCLUDE TOKENIZER IN CLASS\n",
    "    def __init__(self):\n",
    "        super(DistilBertClass, self).__init__()\n",
    "        \n",
    "        ### Import DistilBert Model\n",
    "        distilbert = transformers.DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        \n",
    "        ### Freeze parameters\n",
    "        for param in distilbert.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        ### DistilBert encoder\n",
    "        self.distil = distilbert\n",
    "                        \n",
    "            \n",
    "        self.pre_classifier = nn.Linear(770, 3072) #768\n",
    "        \n",
    "        self.fc2 = nn.Linear(3072, 1024)    \n",
    "        self.fc3 = nn.Linear(1024, 512)\n",
    "        self.fc4 = nn.Linear(512, 128)\n",
    "        \n",
    "        self.classifier = nn.Linear(128, 22)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.01) #.3\n",
    "        \n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, hate, sent):\n",
    "\n",
    "        ### Run DistilBert\n",
    "        distil_output = self.distil(input_ids = input_ids, attention_mask = attention_mask)\n",
    "\n",
    "        ### Grab hidden state\n",
    "        hidden_state = distil_output[0]\n",
    "        x1 = hidden_state[:, 0]\n",
    "        \n",
    "\n",
    "        # prepare hidden state to append additional features (hate and sentiment; hotencoded)\n",
    "        x1 = x1.view(x1.size(0), -1)\n",
    "        \n",
    "        ### Bring in hate and sentiment\n",
    "        x2 = hate\n",
    "        x3 = sent\n",
    "\n",
    "        ### Concatenate hidden state, hate, and sentiment\n",
    "        x = torch.cat((x1, x2, x3), dim=1)\n",
    "\n",
    "        ### Run through linear layers\n",
    "        fc_output = x\n",
    "        fc_output = self.pre_classifier(fc_output)\n",
    "\n",
    "        fc_output = nn.GELU()(fc_output)\n",
    "        fc_output = self.dropout(fc_output)\n",
    "\n",
    "        fc_output = self.fc2(fc_output)\n",
    "        fc_output = nn.GELU()(fc_output)\n",
    "        fc_output = self.dropout(fc_output)\n",
    "\n",
    "        #GELU\n",
    "\n",
    "        fc_output = self.fc3(fc_output)\n",
    "        fc_output = nn.GELU()(fc_output)\n",
    "        fc_output = self.dropout(fc_output)\n",
    "\n",
    "\n",
    "        fc_output = self.fc4(fc_output)\n",
    "        fc_output = nn.Tanh()(fc_output)  # using gelu except for the last one to allow for classification\n",
    "        fc_output = self.dropout(fc_output)\n",
    "\n",
    "        output = self.classifier(fc_output)\n",
    "\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "# \n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define CNN_Distilbert Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet18_DB(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet18_DB, self).__init__()\n",
    "        \n",
    "        ### Import model\n",
    "        resnet18 = models.resnet18(pretrained = True)\n",
    "        \n",
    "        ### Freeze parameters\n",
    "        for param in resnet18.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        ### Change last layer to match output\n",
    "        resnet18.fc = nn.Sequential(\n",
    "            nn.Linear(512, 22),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        ### CNN Encoder\n",
    "        self.cnn = resnet18\n",
    "        \n",
    "        ### Classifier\n",
    "        self.classifier = nn.Linear(22,22)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, images, distil_output):\n",
    "        \n",
    "        ### Pass images through cnn\n",
    "        cnn_output = self.cnn(images)\n",
    "        \n",
    "        ### Compute average between two outputs\n",
    "        x = cnn_output + distil_output / 2\n",
    "        \n",
    "        ### Pass through final linear layer and sigmoid\n",
    "        \n",
    "        x = F.sigmoid(self.classifier(x))\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet50_DB(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet50_DB, self).__init__()\n",
    "        \n",
    "        ### Import model\n",
    "        resnet50 = models.resnet50(pretrained = True)\n",
    "        \n",
    "        ### Freeze parameters\n",
    "        for param in resnet50.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        ### Change last layer to match output\n",
    "        resnet50.fc = nn.Sequential(\n",
    "            nn.Linear(2048, 22),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        ### CNN Encoder\n",
    "        self.cnn = resnet50\n",
    "        \n",
    "        ### Classifier\n",
    "        self.classifier = nn.Linear(22,22)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, images, distil_output):\n",
    "        \n",
    "        ### Pass images through cnn\n",
    "        cnn_output = self.cnn(images)\n",
    "        \n",
    "        ### Compute average between two outputs\n",
    "        x = cnn_output + distil_output / 2\n",
    "        \n",
    "        ### Pass through final linear layer and sigmoid\n",
    "        \n",
    "        x = F.sigmoid(self.classifier(x))\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet101_DB(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet101_DB, self).__init__()\n",
    "        \n",
    "        ### Import model\n",
    "        resnet101 = models.resnet101(pretrained = True)\n",
    "        \n",
    "        ### Freeze parameters\n",
    "        for param in resnet101.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        ### Change last layer to match output\n",
    "        resnet101.fc = nn.Sequential(\n",
    "            nn.Linear(2048, 22),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        ### CNN Encoder\n",
    "        self.cnn = resnet101\n",
    "        \n",
    "        ### Classifier\n",
    "        self.classifier = nn.Linear(22,22)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, images, distil_output):\n",
    "        \n",
    "        ### Pass images through cnn\n",
    "        cnn_output = self.cnn(images)\n",
    "        \n",
    "        ### Compute average between two outputs\n",
    "        x = cnn_output + distil_output / 2\n",
    "        \n",
    "        ### Pass through final linear layer and sigmoid\n",
    "        \n",
    "        x = F.sigmoid(self.classifier(x))\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DenseNet121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet121_DB(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DenseNet121_DB, self).__init__()\n",
    "        \n",
    "        ### Import model\n",
    "        densenet121 = models.densenet121(pretrained = True)\n",
    "        \n",
    "        ### Freeze parameters\n",
    "        for param in densenet121.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        ### Change last layer to match output\n",
    "        densenet121.classifier = nn.Sequential(\n",
    "            nn.Linear(1024, 22),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        ### CNN Encoder\n",
    "        self.cnn = densenet121\n",
    "        \n",
    "        ### Classifier\n",
    "        self.classifier = nn.Linear(22,22)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, images, distil_output):\n",
    "        \n",
    "        ### Pass images through cnn\n",
    "        cnn_output = self.cnn(images)\n",
    "        \n",
    "        ### Compute average between two outputs\n",
    "        x = cnn_output + distil_output / 2\n",
    "        \n",
    "        ### Pass through final linear layer and sigmoid\n",
    "        \n",
    "        x = F.sigmoid(self.classifier(x))\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DenseNet169"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet169_DB(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DenseNet169_DB, self).__init__()\n",
    "        \n",
    "        ### Import model\n",
    "        densenet169 = models.densenet169(pretrained = True)\n",
    "        \n",
    "        ### Freeze parameters\n",
    "        for param in densenet169.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        ### Change last layer to match output\n",
    "        densenet169.classifier = nn.Sequential(\n",
    "            nn.Linear(1664, 22),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        ### CNN Encoder\n",
    "        self.cnn = densenet169\n",
    "        \n",
    "        ### Classifier\n",
    "        self.classifier = nn.Linear(22,22)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, images, distil_output):\n",
    "        \n",
    "        ### Pass images through cnn\n",
    "        cnn_output = self.cnn(images)\n",
    "        \n",
    "        ### Compute average between two outputs\n",
    "        x = cnn_output + distil_output / 2\n",
    "        \n",
    "        ### Pass through final linear layer and sigmoid\n",
    "        \n",
    "        x = F.sigmoid(self.classifier(x))\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DenseNet201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet201_DB(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DenseNet201_DB, self).__init__()\n",
    "        \n",
    "        ### Import model\n",
    "        densenet201 = models.densenet201(pretrained = True)\n",
    "        \n",
    "        ### Freeze parameters\n",
    "        for param in densenet201.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        ### Change last layer to match output\n",
    "        densenet201.classifier = nn.Sequential(\n",
    "            nn.Linear(1920, 22),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        ### CNN Encoder\n",
    "        self.cnn = densenet201\n",
    "        \n",
    "        ### Classifier\n",
    "        self.classifier = nn.Linear(22,22)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, images, distil_output):\n",
    "        \n",
    "        ### Pass images through cnn\n",
    "        cnn_output = self.cnn(images)\n",
    "        \n",
    "        ### Compute average between two outputs\n",
    "        x = cnn_output + distil_output / 2\n",
    "        \n",
    "        ### Pass through final linear layer and sigmoid\n",
    "        \n",
    "        x = F.sigmoid(self.classifier(x))\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG11_BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG11_BN_DB(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG11_BN_DB, self).__init__()\n",
    "        \n",
    "        ### Import model\n",
    "        vgg11bn = models.vgg11_bn(pretrained = True)\n",
    "        \n",
    "        ### Freeze parameters\n",
    "        for param in vgg11bn.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        ### Change last layer to match output\n",
    "        vgg11bn.classifier[6] = nn.Sequential(\n",
    "            nn.Linear(4096, 22),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        ### CNN Encoder\n",
    "        self.cnn = vgg11bn\n",
    "        \n",
    "        ### Classifier\n",
    "        self.classifier = nn.Linear(22,22)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, images, distil_output):\n",
    "        \n",
    "        ### Pass images through cnn\n",
    "        cnn_output = self.cnn(images)\n",
    "        \n",
    "        ### Compute average between two outputs\n",
    "        x = cnn_output + distil_output / 2\n",
    "        \n",
    "        ### Pass through final linear layer and sigmoid\n",
    "        \n",
    "        x = F.sigmoid(self.classifier(x))\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG16_BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16_BN_DB(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG16_BN_DB, self).__init__()\n",
    "        \n",
    "        ### Import model\n",
    "        vgg16bn = models.vgg16_bn(pretrained = True)\n",
    "        \n",
    "        ### Freeze parameters\n",
    "        for param in vgg16bn.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        ### Change last layer to match output\n",
    "        vgg16bn.classifier[6] = nn.Sequential(\n",
    "            nn.Linear(4096, 22),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        ### CNN Encoder\n",
    "        self.cnn = vgg16bn\n",
    "        \n",
    "        ### Classifier\n",
    "        self.classifier = nn.Linear(22,22)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, images, distil_output):\n",
    "        \n",
    "        ### Pass images through cnn\n",
    "        cnn_output = self.cnn(images)\n",
    "        \n",
    "        ### Compute average between two outputs\n",
    "        x = cnn_output + distil_output / 2\n",
    "        \n",
    "        ### Pass through final linear layer and sigmoid\n",
    "        \n",
    "        x = F.sigmoid(self.classifier(x))\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG19_BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG19_BN_DB(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG19_BN_DB, self).__init__()\n",
    "        \n",
    "        ### Import model\n",
    "        vgg19bn = models.vgg19_bn(pretrained = True)\n",
    "        \n",
    "        ### Freeze parameters\n",
    "        for param in vgg19bn.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        ### Change last layer to match output\n",
    "        vgg19bn.classifier[6] = nn.Sequential(\n",
    "            nn.Linear(4096, 22),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        ### CNN Encoder\n",
    "        self.cnn = vgg19bn\n",
    "        \n",
    "        ### Classifier\n",
    "        self.classifier = nn.Linear(22,22)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, images, distil_output):\n",
    "        \n",
    "        ### Pass images through cnn\n",
    "        cnn_output = self.cnn(images)\n",
    "        \n",
    "        ### Compute average between two outputs\n",
    "        x = cnn_output + distil_output / 2\n",
    "        \n",
    "        ### Pass through final linear layer and sigmoid\n",
    "        \n",
    "        x = F.sigmoid(self.classifier(x))\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "# \n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training Methodology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(optim):\n",
    "    for param_group in optim.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This function is used to train a cnn model\n",
    "def Kfold_train_CNN_DB(CNN = None, DistilBert = None, training_data = None, learning_rate = None, k_folds = None, n_epochs = None, model_name = None):\n",
    "    \n",
    "    ### Check that all entries are valid\n",
    "    if ((CNN == None) or (training_data == None) or (model_name == None) or \n",
    "        (learning_rate == None) or (k_folds == None) or (n_epochs == None) or (DistilBert == None)):\n",
    "        print ('Enter all info.')\n",
    "        \n",
    "        \n",
    "        \n",
    "    ### Run K-Fold CV\n",
    "    else:\n",
    "        \n",
    "        device = 'cpu'\n",
    "\n",
    "        ### Set Loss Function and Optimizer\n",
    "        criterion = nn.BCELoss()\n",
    "        \n",
    "        \n",
    "        #### Define the K-fold Cross Validator\n",
    "        kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "        ### Create values to hold the best model metrics across folds\n",
    "        val_f1_mic_max = 0 ### This determines best model\n",
    "        \n",
    "        best_train_loss = 0\n",
    "        best_train_acc = 0\n",
    "        best_train_f1_mic = 0\n",
    "        best_train_f1_mac = 0\n",
    "        best_train_prec_mic = 0\n",
    "        best_train_prec_mac = 0\n",
    "        best_train_rec_mic = 0\n",
    "        best_train_rec_mac = 0\n",
    "\n",
    "        best_val_acc = 0\n",
    "        best_val_loss = 0\n",
    "#         best_val_f1_mic = 0\n",
    "        best_val_f1_mac = 0\n",
    "        best_val_prec_mic = 0\n",
    "        best_val_prec_mac = 0\n",
    "        best_val_rec_mic = 0\n",
    "        best_val_rec_mac = 0\n",
    "        \n",
    "        best_fold = 0\n",
    "        best_epoch = 0\n",
    "        \n",
    "\n",
    "        \n",
    "        ### Start print\n",
    "\n",
    "        start = time.time()\n",
    "        \n",
    "        ### K-fold Cross Validation model evaluation\n",
    "        for fold, (train_ids, val_ids) in enumerate(kfold.split(training_data)):\n",
    "            \n",
    "            print('-------------------------------------------')\n",
    "            print('FOLD {}'.format(fold + 1))\n",
    "            print('-------------------------------------------')\n",
    "            \n",
    "            ### Sample elements randomly from a given list of ids, no replacement\n",
    "            train_subsampler = SubsetRandomSampler(train_ids)\n",
    "            val_subsampler = SubsetRandomSampler(val_ids)\n",
    "            \n",
    "            ### Define data loaders for training and validation in current fold\n",
    "            train_loader = DataLoader(dataset = training_data, batch_size = 25, sampler = train_subsampler)\n",
    "            val_loader = DataLoader(dataset = training_data, batch_size = 25, sampler = val_subsampler)\n",
    "            \n",
    "            ### Initialize network\n",
    "            network1 = DistilBert\n",
    "            network2 = CNN\n",
    "            if torch.cuda.is_available():\n",
    "                network1.cuda()\n",
    "                network1 = nn.DataParallel(network1, list(range(2)))\n",
    "                \n",
    "                network2.cuda()\n",
    "                network2 = nn.DataParallel(network2, list(range(2)))\n",
    "                \n",
    "                device = 'cuda'\n",
    "            \n",
    "            ### Initialize optimizer\n",
    "            optimizer1 = transformers.Adafactor(\n",
    "                network1.parameters(),\n",
    "                lr=None,  # when using warm up and relative step, LR is auto determined\n",
    "                eps=(1e-30, 1e-3),\n",
    "                clip_threshold=1.0,\n",
    "                decay_rate=-0.8, #0.8\n",
    "                beta1=None, # <- used for L1 regularization\n",
    "                weight_decay=0.000002, # L2 regularization, to prevent overfitting  (beta2)\n",
    "                relative_step=True,\n",
    "                scale_parameter=True, # https://github.com/pytorch/pytorch/issues/25081 this setting keeps the gradients from reaching 0 (using the clip threshold) (if this is enabled, must modify in training)\n",
    "                warmup_init=True\n",
    "            )\n",
    "                        \n",
    "            optimizer2 = optim.Adam(network2.parameters(), lr=learning_rate)\n",
    "            scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer2, mode = 'min', factor = 0.1, patience = 5)\n",
    "            \n",
    "            \n",
    "            ### Create lists of values at the end of each epoch for each fold\n",
    "            train_loss = []\n",
    "            train_acc = []\n",
    "            train_f1_mic = []\n",
    "            train_f1_mac = []\n",
    "            train_prec_mic = []\n",
    "            train_prec_mac = []\n",
    "            train_rec_mic = []\n",
    "            train_rec_mac = []\n",
    "            \n",
    "            val_loss = []\n",
    "            val_acc = []\n",
    "            val_f1_mic = []\n",
    "            val_f1_mac = []\n",
    "            val_prec_mic = []\n",
    "            val_prec_mac = []\n",
    "            val_rec_mic = []\n",
    "            val_rec_mac = []\n",
    "            \n",
    "            \n",
    "            ### Train network\n",
    "            for epoch in range(n_epochs):\n",
    "                \n",
    "                ### Hold training predictions and targets\n",
    "                train_output = np.empty((0,22), int)\n",
    "                train_all_targets = np.empty((0,22), int)\n",
    "                \n",
    "                val_output = np.empty((0,22), int)\n",
    "                val_all_targets = np.empty((0,22), int)\n",
    "                \n",
    "                \n",
    "                ### Train ###\n",
    "                network1.train()\n",
    "                network2.train()\n",
    "                \n",
    "                train_running_loss = 0.0\n",
    "                \n",
    "                batch_number = 0\n",
    "                for i, data in enumerate(train_loader):\n",
    "                    \n",
    "                    images, text, hate, sent, targets = data[0].to(device), data[1], data[2].to(device), data[3].to(device), data[4].float().to(device)\n",
    "                    \n",
    "                    optimizer1.zero_grad()\n",
    "                    optimizer2.zero_grad()\n",
    "                    \n",
    "                    \n",
    "                    ids = []\n",
    "                    mask = []\n",
    "\n",
    "                    for j in text:   \n",
    "                        inputs = tokenizer.encode_plus(\n",
    "                                j, \n",
    "                                None,\n",
    "                                add_special_tokens = True,    \n",
    "                                max_length= 512,\n",
    "                                padding = \"max_length\",\n",
    "                                pad_to_max_length = True,\n",
    "                                return_token_type_ids= False)\n",
    "\n",
    "                        ids.append(inputs['input_ids'])\n",
    "                        mask.append(inputs['attention_mask'])\n",
    "\n",
    "                    ids = torch.from_numpy(np.array(ids)).to(device)\n",
    "                    mask = torch.from_numpy(np.array(mask)).to(device)\n",
    "\n",
    "                    distil_output = network1(ids, mask, hate, sent)\n",
    "\n",
    "                    output = network2(images, distil_output)\n",
    "\n",
    "                    \n",
    "                    loss = criterion(output, targets) \n",
    "                    train_running_loss += loss.item()\n",
    "                                        \n",
    "                    ### Append output\n",
    "                    train_output = np.vstack((train_output, ((output > 0.5).cpu().numpy().astype('int'))))\n",
    "                    train_all_targets = np.vstack((train_all_targets, targets.cpu().numpy().astype('int')))\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizer1.step()\n",
    "                    optimizer2.step()\n",
    "                    \n",
    "                    \n",
    "                ### Calculate metrics and append\n",
    "                train_loss.append(train_running_loss/len(train_loader.dataset))\n",
    "                train_acc.append(accuracy_score(train_all_targets, train_output))\n",
    "                train_f1_mic.append(f1_score(train_all_targets, train_output, average = 'micro'))\n",
    "                train_f1_mac.append(f1_score(train_all_targets, train_output, average = 'macro'))\n",
    "                train_prec_mic.append(precision_score(train_all_targets, train_output, average = 'micro'))\n",
    "                train_prec_mac.append(precision_score(train_all_targets, train_output, average = 'macro'))\n",
    "                train_rec_mic.append(recall_score(train_all_targets, train_output, average = 'micro'))\n",
    "                train_rec_mac.append(recall_score(train_all_targets, train_output, average = 'macro'))\n",
    "                \n",
    "                \n",
    "                ### Validate###\n",
    "                network1.eval()\n",
    "                network2.eval()\n",
    "                \n",
    "                val_running_loss = 0.0\n",
    "\n",
    "                                \n",
    "                for i, data in enumerate(val_loader):\n",
    "                    images, text, hate, sent, targets = data[0].to(device), data[1], data[2].to(device), data[3].to(device), data[4].float().to(device)\n",
    "                    \n",
    "                    ids = []\n",
    "                    mask = []\n",
    "                    \n",
    "                    for j in text:   \n",
    "                        inputs = tokenizer.encode_plus(\n",
    "                                j, \n",
    "                                None,\n",
    "                                add_special_tokens = True,    \n",
    "                                max_length= 512,\n",
    "                                padding = \"max_length\",\n",
    "                                pad_to_max_length = True,\n",
    "                                return_token_type_ids= False)\n",
    "\n",
    "                        ids.append(inputs['input_ids'])\n",
    "                        mask.append(inputs['attention_mask'])\n",
    "\n",
    "                    ids = torch.from_numpy(np.array(ids)).to(device)\n",
    "                    mask = torch.from_numpy(np.array(mask)).to(device)\n",
    "\n",
    "                    distil_output = network1(ids, mask, hate, sent)\n",
    "\n",
    "                    output = network2(images, distil_output)\n",
    "                    \n",
    "                    loss = criterion(output, targets)\n",
    "                    val_running_loss += loss.item()\n",
    "                    \n",
    "                    ### Append output\n",
    "                    val_output = np.vstack((val_output, ((output > 0.5).cpu().numpy().astype('int'))))\n",
    "                    val_all_targets = np.vstack((val_all_targets, targets.cpu().numpy().astype('int')))\n",
    "\n",
    "                \n",
    "                ### Calculate metrics and append\n",
    "                val_loss.append(val_running_loss/len(val_loader.dataset))\n",
    "                val_acc.append(accuracy_score(val_all_targets, val_output))\n",
    "                val_f1_mic.append(f1_score(val_all_targets, val_output, average = 'micro'))\n",
    "                val_f1_mac.append(f1_score(val_all_targets, val_output, average = 'macro'))\n",
    "                val_prec_mic.append(precision_score(val_all_targets, val_output, average = 'micro'))\n",
    "                val_prec_mac.append(precision_score(val_all_targets, val_output, average = 'macro'))\n",
    "                val_rec_mic.append(recall_score(val_all_targets, val_output, average = 'micro'))\n",
    "                val_rec_mac.append(recall_score(val_all_targets, val_output, average = 'macro'))\n",
    "                \n",
    "                \n",
    "                ### Save model with the lowest validation loss\n",
    "                if val_f1_mic[epoch] > val_f1_mic_max:\n",
    "                    print('Validation F1 Micro Score Increased ({:.6f} --> {:.6f}).  Saving model ...\\n'.format(\n",
    "                    val_f1_mic_max,\n",
    "                    val_f1_mic[epoch]))\n",
    "                    torch.save(network1.state_dict(), 'BestCnnDBModels/DB_' + model_name[:-3] + '.pt')\n",
    "                    torch.save(network2.state_dict(), 'BestCnnDBModels/' + model_name +'.pt')\n",
    "                    val_f1_mic_max = val_f1_mic[epoch]\n",
    "                    \n",
    "                    ### Set current best metrics\n",
    "                    best_train_loss = train_loss[epoch]\n",
    "                    best_train_acc = train_acc[epoch]\n",
    "                    best_train_f1_mic = train_f1_mic[epoch]\n",
    "                    best_train_f1_mac = train_f1_mac[epoch]\n",
    "                    best_train_prec_mic = train_prec_mic[epoch]\n",
    "                    best_train_prec_mac = train_prec_mac[epoch]\n",
    "                    best_train_rec_mic = train_rec_mic[epoch]\n",
    "                    best_train_rec_mac = train_rec_mac[epoch]\n",
    "\n",
    "                    best_val_acc = val_acc[epoch]\n",
    "                    best_val_loss = val_loss[epoch]\n",
    "#                     best_val_f1_mic = val_f1_mic[epoch]\n",
    "                    best_val_f1_mac = val_f1_mac[epoch]\n",
    "                    best_val_prec_mic = val_prec_mic[epoch]\n",
    "                    best_val_prec_mac = val_prec_mac[epoch]\n",
    "                    best_val_rec_mic = val_rec_mic[epoch]\n",
    "                    best_val_rec_mac = val_rec_mac[epoch]\n",
    "\n",
    "                    best_fold = fold + 1\n",
    "                    best_epoch = epoch + 1\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                ### Display summary for epoch\n",
    "                print('Epoch {} \\tLearning Rate: {} \\tTime (min): {}'.format(epoch+1, get_lr(optimizer2), round((time.time()-start)/60, 2)))\n",
    "                print('Train Loss: {} \\tValidation Loss: {}'. format(round(train_loss[epoch], 4),\n",
    "                                                                     round(val_loss[epoch], 4)))\n",
    "                print('Train Accuracy: {} \\tValidation Accuracy: {}'.format(round(train_acc[epoch], 4),\n",
    "                                                                            round(val_acc[epoch], 4)))\n",
    "                print('Train F1 Mirco: {} \\tValidation F1 Micro: {}'.format(round(train_f1_mic[epoch], 4),\n",
    "                                                                            round(val_f1_mic[epoch], 4)))\n",
    "                print('Train F1 Marco: {} \\tValidation F1 Macro: {}'.format(round(train_f1_mac[epoch], 4),\n",
    "                                                                            round(val_f1_mac[epoch], 4)))\n",
    "                print('Train Precision Mirco: {} \\tValidation Precision Micro: {}'.format(round(train_prec_mic[epoch], 4),\n",
    "                                                                                          round(val_prec_mic[epoch], 4)))\n",
    "                print('Train Precision Marco: {} \\tValidation Precision Macro: {}'.format(round(train_prec_mac[epoch], 4),\n",
    "                                                                                          round(val_prec_mac[epoch], 4)))\n",
    "                print('Train Recall Mirco: {} \\tValidation Recall Micro: {}'.format(round(train_rec_mic[epoch], 4),\n",
    "                                                                                    round(val_rec_mic[epoch], 4)))\n",
    "                print('Train Recall Marco: {} \\tValidation Recall Macro: {}\\n'.format(round(train_rec_mac[epoch], 4),\n",
    "                                                                                    round(val_rec_mac[epoch], 4)))\n",
    "                \n",
    "                \n",
    "                ### Update learning rate if needed\n",
    "                scheduler.step(val_loss[epoch])\n",
    "                \n",
    "                \n",
    "                \n",
    "            ### Display summary graph of fold\n",
    "            fig, (ax1, ax3) = plt.subplots(1,2, figsize = (20,6))\n",
    "            ax1.set_xlabel('Epoch')\n",
    "            ax1.set_ylabel('Loss')\n",
    "            ln1 = ax1.plot(np.arange(start = 1, stop = n_epochs + 1), train_loss, label = 'Train Loss')\n",
    "            ln2 = ax1.plot(np.arange(start = 1, stop = n_epochs + 1), val_loss, label = 'Val Loss')\n",
    "            \n",
    "            ax2 = ax1.twinx()\n",
    "            ax2.set_ylabel('Accuracy')\n",
    "            ln3 = ax2.plot(np.arange(start = 1, stop = n_epochs + 1), train_acc, marker = 'o', label = 'Train Acc')\n",
    "            ln4 = ax2.plot(np.arange(start = 1, stop = n_epochs + 1), val_acc, marker = 'o', label = 'Val Acc')\n",
    "            \n",
    "            lns1 = ln1 + ln2 + ln3 + ln4\n",
    "            labs1 = [l.get_label() for l in lns1]\n",
    "            \n",
    "            ax3.set_xlabel('Epoch')\n",
    "            ax3.set_ylabel('Score')\n",
    "            ln5 = ax3.plot(np.arange(start = 1, stop = n_epochs + 1), train_f1_mic, marker = 'v', label = 'Train F1 Micro')\n",
    "            ln6 = ax3.plot(np.arange(start = 1, stop = n_epochs + 1), val_f1_mic, marker = 'v', label = 'Val F1 Micro')\n",
    "            ln7 = ax3.plot(np.arange(start = 1, stop = n_epochs + 1), train_f1_mac, marker = '^', label = 'Train F1 Macro')\n",
    "            ln8 = ax3.plot(np.arange(start = 1, stop = n_epochs + 1), val_f1_mac, marker = '^', label = 'Val F1 Micro')\n",
    "            ln9 = ax3.plot(np.arange(start = 1, stop = n_epochs + 1), train_prec_mic, marker = 'd', label = 'Train Prec. Micro')\n",
    "            ln10 = ax3.plot(np.arange(start = 1, stop = n_epochs + 1), val_prec_mic, marker = 'd', label = 'Val Prec. Micro')\n",
    "            ln11 = ax3.plot(np.arange(start = 1, stop = n_epochs + 1), train_prec_mac, marker = 'X', label = 'Train Prec. Macro')\n",
    "            ln12 = ax3.plot(np.arange(start = 1, stop = n_epochs + 1), val_prec_mac, marker = 'X', label = 'Val Prec. Macro')\n",
    "            ln13 = ax3.plot(np.arange(start = 1, stop = n_epochs + 1), train_rec_mic, marker = 'P', label = 'Train Rec. Micro')\n",
    "            ln14 = ax3.plot(np.arange(start = 1, stop = n_epochs + 1), val_rec_mic, marker = 'P', label = 'Val Rec. Micro')\n",
    "            ln15 = ax3.plot(np.arange(start = 1, stop = n_epochs + 1), train_rec_mac, marker = 's', label = 'Train Rec. Macro')\n",
    "            ln16 = ax3.plot(np.arange(start = 1, stop = n_epochs + 1), val_rec_mac, marker = 's', label = 'Val Rec. Macro')\n",
    "            \n",
    "            lns2 = ln5 + ln6 + ln7 + ln8 +  ln9 + ln10 + ln11 + ln12 + ln13 + ln14 + ln15 + ln16\n",
    "            labs2 = [l.get_label() for l in lns2]\n",
    "            \n",
    "            \n",
    "            ax1.legend(lns1, labs1, loc = 'upper left', bbox_to_anchor = (1.1,1))\n",
    "            ax3.legend(lns2, labs2, loc = 'upper left', bbox_to_anchor = (1.05,1))\n",
    "            fig.tight_layout()\n",
    "            \n",
    "            plt.show()\n",
    "            \n",
    "            fig.savefig('MultiModal_IndLabels_TrainingSummary/' + model_name + '_FOLD' + str(fold+1) + '.png')\n",
    "        \n",
    "        \n",
    "        \n",
    "        ### Display metrics of the best model\n",
    "                \n",
    "        print('------------------------------------------------------------')\n",
    "        print('------------------------------------------------------------')\n",
    "        \n",
    "        print('\\nMetrics of Best Model:')\n",
    "        print('Fold: {} \\tEpoch: {}'.format(best_fold, best_epoch))\n",
    "        print('Train Loss: {} \\tValidation Loss: {}'. format(round(best_train_loss, 4),\n",
    "                                                             round(best_val_loss, 4)))\n",
    "        print('Train Accuracy: {} \\tValidation Accuracy: {}'.format(round(best_train_acc, 4),\n",
    "                                                                    round(best_val_acc, 4)))\n",
    "        print('Train F1 Mirco: {} \\tValidation F1 Micro: {}'.format(round(best_train_f1_mic, 4),\n",
    "                                                                    round(val_f1_mic_max, 4)))\n",
    "        print('Train F1 Marco: {} \\tValidation F1 Macro: {}'.format(round(best_train_f1_mac, 4),\n",
    "                                                                    round(best_val_f1_mac, 4)))\n",
    "        print('Train Precision Mirco: {} \\tValidation Precision Micro: {}'.format(round(best_train_prec_mic, 4),\n",
    "                                                                                  round(best_val_prec_mic, 4)))\n",
    "        print('Train Precision Marco: {} \\tValidation Precision Macro: {}'.format(round(best_train_prec_mac, 4),\n",
    "                                                                                  round(best_val_prec_mac, 4)))\n",
    "        print('Train Recall Mirco: {} \\tValidation Recall Micro: {}'.format(round(best_train_rec_mic, 4),\n",
    "                                                                            round(best_val_rec_mic, 4)))\n",
    "        print('Train Recall Marco: {} \\tValidation Recall Macro: {}'.format(round(best_train_rec_mac, 4),\n",
    "                                                                            round(best_val_rec_mac, 4)))\n",
    "        \n",
    "        \n",
    "        ### Return best metrics\n",
    "        return [best_fold, best_epoch, best_train_loss, best_train_acc, best_train_f1_mic, best_train_f1_mac, best_train_prec_mic, best_train_prec_mac, best_train_rec_mic, best_train_rec_mac,\n",
    "                best_val_loss, best_val_acc, val_f1_mic_max, best_val_f1_mac, best_val_prec_mic, best_val_prec_mac, best_val_rec_mic, best_val_rec_mac]\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "# \n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_resnet18 = DistilBertClass()\n",
    "DB_resnet50 = DistilBertClass()\n",
    "DB_resnet101 = DistilBertClass()\n",
    "\n",
    "DB_densenet121 = DistilBertClass()\n",
    "DB_densenet169 = DistilBertClass()\n",
    "DB_densenet201 = DistilBertClass()\n",
    "\n",
    "DB_vgg11bn = DistilBertClass()\n",
    "DB_vgg16bn = DistilBertClass()\n",
    "DB_vgg19bn = DistilBertClass()\n",
    "\n",
    "resnet18 = ResNet18_DB()\n",
    "resnet50 = ResNet50_DB()\n",
    "resnet101 = ResNet101_DB()\n",
    "\n",
    "densenet121 = DenseNet121_DB()\n",
    "densenet169 = DenseNet169_DB()\n",
    "densenet201 = DenseNet201_DB()\n",
    "\n",
    "vgg11bn = VGG11_BN_DB()\n",
    "vgg16bn = VGG16_BN_DB()\n",
    "vgg19bn = VGG19_BN_DB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "# \n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18_DB_best = Kfold_train_CNN_DB(CNN = resnet18, DistilBert = DB_resnet18,\n",
    "                                       training_data = training_data, learning_rate = 0.5, \n",
    "                                       k_folds = 10, n_epochs = 30, model_name = 'resnet18_DB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50_DB_best = Kfold_train_CNN_DB(CNN = resnet50, DistilBert = DB_resnet50, \n",
    "                                      training_data = training_data, learning_rate = 0.01,\n",
    "                                      k_folds = 10, n_epochs = 30, model_name = 'resnet50_DB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet101_DB_best = Kfold_train_CNN_DB(CNN = resnet101, DistilBert = DB_resnet101, \n",
    "                                       training_data = training_data, learning_rate = 0.01,\n",
    "                                       k_folds = 10, n_epochs = 30, model_name = 'resnet101_DB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "densenet121_DB_best = Kfold_train_CNN_DB(CNN = densenet121, DistilBert = DB_densenet121, \n",
    "                                         training_data = training_data, learning_rate = 0.01,\n",
    "                                         k_folds = 10, n_epochs = 30, model_name = 'densenet121_DB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "densenet169_DB_best = Kfold_train_CNN_DB(CNN = densenet169, DistilBert = DB_densenet169, \n",
    "                                         training_data = training_data, learning_rate = 0.01, \n",
    "                                         k_folds = 10, n_epochs = 30, model_name = 'densenet169_DB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "densenet201_DB_best = Kfold_train_CNN_DB(CNN = densenet201, DistilBert = DB_densenet201, \n",
    "                                         training_data = training_data, learning_rate = 0.01, \n",
    "                                         k_folds = 10, n_epochs = 30, model_name = 'densenet201_DB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg11bn_DB_best = Kfold_train_CNN_DB(CNN = vgg11bn, DistilBert = DB_vgg11bn, \n",
    "                                     training_data = training_data, learning_rate = 0.01, \n",
    "                                     k_folds = 10, n_epochs = 30, model_name = 'vgg11bn_DB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16bn_DB_best = Kfold_train_CNN_DB(CNN = vgg16bn, DistilBert = DB_vgg16bn, \n",
    "                                     training_data = training_data, learning_rate = 0.01, \n",
    "                                     k_folds = 10, n_epochs = 30, model_name = 'vgg16bn_DB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg19bn_DB_best = Kfold_train_CNN_DB(CNN = vgg19bn, DistilBert = DB_vgg19bn, \n",
    "                                     training_data = training_data, learning_rate = 0.01, \n",
    "                                     k_folds = 10, n_epochs = 30, model_name = 'vgg19bn_DB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total Training Time (HR:M:S): ' + str(datetime.timedelta(hours = ((time.time() - START) / 60 / 60))).rsplit('.', 1)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "# \n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaryDF = pd.DataFrame({\n",
    "    'ResNet18' : resnet18_DB_best,\n",
    "    'ResNet50' : resnet50_DB_best,\n",
    "    'ResNet101' : resnet101_DB_best,\n",
    "    'DenseNet121' : densenet121_DB_best,\n",
    "    'DenseNet169' : densenet169_DB_best,\n",
    "    'DenseNet201' : densenet201_DB_best,\n",
    "    'VGG11_BN' : vgg11bn_DB_best,\n",
    "    'VGG16_BN' : vgg16bn_DB_best,\n",
    "    'VGG19_BN' : vgg19bn_DB_best\n",
    "})\n",
    "summaryDF.index = ['Fold', 'Epoch', 'Train Loss', 'Train Accuracy', 'Train F1 Micro', 'Train F1 Macro', 'Train Precision Micro', 'Train Precision Macro', 'Train Recall Micro', 'Train Recall Macro',\n",
    "                   'Val Loss', 'Val Accuracy', 'Val F1 Micro', 'Val F1 Macro', 'Val Precision Micro', 'Val Precision Macro', 'Val Recall Micro', 'Val Recall Macro']\n",
    "\n",
    "summaryDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Summary Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data = [\n",
    "    go.Bar(name = 'ResNet18', x = summaryDF.index[3:10], y = summaryDF['ResNet18'][3:10]),\n",
    "    go.Bar(name = 'ResNet50', x = summaryDF.index[3:10], y = summaryDF['ResNet50'][3:10]),\n",
    "    go.Bar(name = 'ResNet101', x = summaryDF.index[3:10], y = summaryDF['ResNet101'][3:10]),\n",
    "    go.Bar(name = 'DenseNet121', x = summaryDF.index[3:10], y = summaryDF['DenseNet121'][3:10]),\n",
    "    go.Bar(name = 'DenseNet169', x = summaryDF.index[3:10], y = summaryDF['DenseNet169'][3:10]),\n",
    "    go.Bar(name = 'DenseNet201', x = summaryDF.index[3:10], y = summaryDF['DenseNet201'][3:10]),\n",
    "    go.Bar(name = 'VGG11_BN', x = summaryDF.index[3:10], y = summaryDF['VGG11_BN'][3:10]),\n",
    "    go.Bar(name = 'VGG16_BN', x = summaryDF.index[3:10], y = summaryDF['VGG16_BN'][3:10]),\n",
    "    go.Bar(name = 'VGG19_BN', x = summaryDF.index[3:10], y = summaryDF['VGG19_BN'][3:10]),\n",
    "])\n",
    "fig.update_layout(barmode = 'group',\n",
    "                  title = 'Best Model Metrics Across Folds (Training) - With DistilBert',\n",
    "                  xaxis_title = 'Metrics',\n",
    "                  yaxis_title = 'Score',\n",
    "                  legend_title = 'Models')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data = [\n",
    "    go.Bar(name = 'ResNet18', x = summaryDF.index[11:], y = summaryDF['ResNet18'][11:]),\n",
    "    go.Bar(name = 'ResNet50', x = summaryDF.index[11:], y = summaryDF['ResNet50'][11:]),\n",
    "    go.Bar(name = 'ResNet101', x = summaryDF.index[11:], y = summaryDF['ResNet101'][11:]),\n",
    "    go.Bar(name = 'DenseNet121', x = summaryDF.index[11:], y = summaryDF['DenseNet121'][11:]),\n",
    "    go.Bar(name = 'DenseNet169', x = summaryDF.index[11:], y = summaryDF['DenseNet169'][11:]),\n",
    "    go.Bar(name = 'DenseNet201', x = summaryDF.index[11:], y = summaryDF['DenseNet201'][11:]),\n",
    "    go.Bar(name = 'VGG11_BN', x = summaryDF.index[11:], y = summaryDF['VGG11_BN'][11:]),\n",
    "    go.Bar(name = 'VGG16_BN', x = summaryDF.index[11:], y = summaryDF['VGG16_BN'][11:]),\n",
    "    go.Bar(name = 'VGG19_BN', x = summaryDF.index[11:], y = summaryDF['VGG19_BN'][11:]),\n",
    "])\n",
    "fig.update_layout(barmode = 'group',\n",
    "                  title = 'Best Model Metrics Across Folds (Validation) - With DistilBert',\n",
    "                  xaxis_title = 'Metrics',\n",
    "                  yaxis_title = 'Score',\n",
    "                  legend_title = 'Models')\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
