{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep for MultiModal Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author: Ian Scarff (iscarff123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import imageio\n",
    "import random\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import time\n",
    "import datetime\n",
    "from hatesonar import Sonar # This is the hate speech detection library; it is based on bert\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in reference data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiLabelBinarizer()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Load in reference data\n",
    "\n",
    "with open('training_data_task3.txt') as f: ### Training data\n",
    "    training = json.load(f)\n",
    "    \n",
    "with open('testing_data_task3.txt') as f: ### Testing data\n",
    "    testing = json.load(f)\n",
    "    \n",
    "classes = ['Smears', 'Loaded Language', 'Name calling/Labeling', 'Glittering generalities (Virtue)',\n",
    "               'Appeal to (Strong) Emotions', 'Appeal to fear/prejudice', 'Transfer', 'Doubt',\n",
    "               'Exaggeration/Minimisation', 'Whataboutism', 'Slogans', 'Flag-waving',\n",
    "               \"Misrepresentation of Someone's Position (Straw Man)\", 'Causal Oversimplification',\n",
    "               'Thought-terminating cliché', 'Black-and-white Fallacy/Dictatorship', 'Appeal to authority',\n",
    "               'Reductio ad hitlerum', 'Repetition', 'Obfuscation, Intentional vagueness, Confusion',\n",
    "               'Presenting Irrelevant Data (Red Herring)', 'Bandwagon']\n",
    "\n",
    "### Create Class Binarizer\n",
    "one_hot = MultiLabelBinarizer()\n",
    "one_hot.fit([classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Appeal to (Strong) Emotions', 'Appeal to authority',\n",
       "       'Appeal to fear/prejudice', 'Bandwagon',\n",
       "       'Black-and-white Fallacy/Dictatorship',\n",
       "       'Causal Oversimplification', 'Doubt', 'Exaggeration/Minimisation',\n",
       "       'Flag-waving', 'Glittering generalities (Virtue)',\n",
       "       'Loaded Language',\n",
       "       \"Misrepresentation of Someone's Position (Straw Man)\",\n",
       "       'Name calling/Labeling',\n",
       "       'Obfuscation, Intentional vagueness, Confusion',\n",
       "       'Presenting Irrelevant Data (Red Herring)', 'Reductio ad hitlerum',\n",
       "       'Repetition', 'Slogans', 'Smears', 'Thought-terminating cliché',\n",
       "       'Transfer', 'Whataboutism'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '182',\n",
       "  'labels': ['Exaggeration/Minimisation',\n",
       "   'Name calling/Labeling',\n",
       "   'Smears',\n",
       "   'Transfer'],\n",
       "  'text': 'The most costly errors in all of history\\n\\nWorld Health Organization (WHO)\\nPreliminary investigations conducted by the Chinese authorities have found no clear evidence of human-to-human transmission of the novel #coronavirus (2019-nCoV) identified in #Wuhan, #China\\n14 Jan 2020\\n\\nWORLD NEWS FEBRUARY 3, 2020 / 10:33 PM / 2 MONTHS AGO\\nWHO chief says widespread travel bans not needed to beat China virus\\n',\n",
       "  'image': '182_image.png'},\n",
       " {'id': '366_batch_2',\n",
       "  'labels': ['Causal Oversimplification',\n",
       "   'Loaded Language',\n",
       "   'Name calling/Labeling'],\n",
       "  'text': 'MY PARENTS WERE KILLED AS A RESULT OF A GUN FREE ZONE.\\n\\nI CARRIED A GUN AT ALL TIMES TILL IT WAS MADE A FELONY TO CARRY IN CERTAIN AREAS. \\nA MAN WITH A GUN STARTED SHOOTING PEOPLE...\\nI HAD THE PERFECT SHOT BUT WAS DISARMED BY LAWS.\\n\\n',\n",
       "  'image': '366_image_batch_2.png'},\n",
       " {'id': '148',\n",
       "  'labels': ['Loaded Language', 'Repetition', 'Slogans', 'Smears'],\n",
       "  'text': \"VOTE IT OUT\\nVOTE IT OUT\\n\\nHE SAID IT WASN'T REAL.\\nHE SAID IT'S CONTAINED.\\nHE SAID IT WOULD DISAPPEAR.\\nHE BLAMED DEMOCRATS.\\nHE SHARED HIS HUNCHES.\\nHE OVERRULED HIS SCIENTISTS.\\nHE HAS FAILED.\\nHE HAS ENDANGERED\\nEVERY ONE OF US.\\n\\nVOTE IT OUT\\nVOTE IT OUT\\n\",\n",
       "  'image': '148_image.png'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "727"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '566_batch_2',\n",
       "  'labels': ['Appeal to (Strong) Emotions', 'Smears'],\n",
       "  'text': \"A FRIENDLY REMINDER...\\n\\nFERGUSON\\n\\nBALTIMORE\\n\\nMILWAUKEE\\n\\nCHARLOTTE\\n\\nIN CASE Y'ALL FORGOT !\\n\",\n",
       "  'image': '566_image_batch_2.png'},\n",
       " {'id': '738_batch_2',\n",
       "  'labels': ['Black-and-white Fallacy/Dictatorship',\n",
       "   'Flag-waving',\n",
       "   'Loaded Language',\n",
       "   'Name calling/Labeling'],\n",
       "  'text': \"We're fed up.\\nWe're not asking for our country back.\\nWe're TAKING IT BACK\\nWE THE PEOPLE ARE COMING\\nAmerica was founded by pissed off people.\\nAnd they're pissed again!\",\n",
       "  'image': '738_image_batch_2.png'},\n",
       " {'id': '885_batch_2',\n",
       "  'labels': ['Smears'],\n",
       "  'text': \"PRESIDENTS' DAY SALE\\n\\nEVERYONE MUST GO\",\n",
       "  'image': '885_image_batch_2.png'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "914"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training) + len(testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Image Names, Text, & Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name_train = []\n",
    "text_train = []\n",
    "one_hot_labels_train = [] ### List to hold one hot labels\n",
    "\n",
    "for obsv in training: ### Go through the training data\n",
    "\n",
    "    ### Images ###\n",
    "    image_name_train.append(obsv['image'])\n",
    "    \n",
    "    text_train.append(obsv['text'])\n",
    "    \n",
    "    ### Labels\n",
    "    one_hot_labels_train.append(one_hot.transform([obsv['labels']])[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['182_image.png', '366_image_batch_2.png', '148_image.png']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_name_train[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The most costly errors in all of history\\n\\nWorld Health Organization (WHO)\\nPreliminary investigations conducted by the Chinese authorities have found no clear evidence of human-to-human transmission of the novel #coronavirus (2019-nCoV) identified in #Wuhan, #China\\n14 Jan 2020\\n\\nWORLD NEWS FEBRUARY 3, 2020 / 10:33 PM / 2 MONTHS AGO\\nWHO chief says widespread travel bans not needed to beat China virus\\n',\n",
       " 'MY PARENTS WERE KILLED AS A RESULT OF A GUN FREE ZONE.\\n\\nI CARRIED A GUN AT ALL TIMES TILL IT WAS MADE A FELONY TO CARRY IN CERTAIN AREAS. \\nA MAN WITH A GUN STARTED SHOOTING PEOPLE...\\nI HAD THE PERFECT SHOT BUT WAS DISARMED BY LAWS.\\n\\n',\n",
       " \"VOTE IT OUT\\nVOTE IT OUT\\n\\nHE SAID IT WASN'T REAL.\\nHE SAID IT'S CONTAINED.\\nHE SAID IT WOULD DISAPPEAR.\\nHE BLAMED DEMOCRATS.\\nHE SHARED HIS HUNCHES.\\nHE OVERRULED HIS SCIENTISTS.\\nHE HAS FAILED.\\nHE HAS ENDANGERED\\nEVERY ONE OF US.\\n\\nVOTE IT OUT\\nVOTE IT OUT\\n\"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_train[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0]),\n",
       " array([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_labels_train[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name_test = []\n",
    "text_test = []\n",
    "one_hot_labels_test = [] ### List to hold one hot labels\n",
    "\n",
    "for obsv in testing: ### Go through the test data\n",
    "\n",
    "    ### Images ###\n",
    "    image_name_test.append(obsv['image'])\n",
    "    \n",
    "    text_test.append(obsv['text'])\n",
    "    \n",
    "    ### Labels\n",
    "    one_hot_labels_test.append(one_hot.transform([obsv['labels']])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['566_image_batch_2.png', '738_image_batch_2.png', '885_image_batch_2.png']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_name_test[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"A FRIENDLY REMINDER...\\n\\nFERGUSON\\n\\nBALTIMORE\\n\\nMILWAUKEE\\n\\nCHARLOTTE\\n\\nIN CASE Y'ALL FORGOT !\\n\",\n",
       " \"We're fed up.\\nWe're not asking for our country back.\\nWe're TAKING IT BACK\\nWE THE PEOPLE ARE COMING\\nAmerica was founded by pissed off people.\\nAnd they're pissed again!\",\n",
       " \"PRESIDENTS' DAY SALE\\n\\nEVERYONE MUST GO\"]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_test[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_labels_test[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ian/.local/lib/python3.6/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.linear_model.logistic module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.linear_model. Anything that cannot be imported from sklearn.linear_model is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/ian/.local/lib/python3.6/site-packages/sklearn/base.py:334: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.19.1 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/ian/.local/lib/python3.6/site-packages/sklearn/base.py:334: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.19.1 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/ian/.local/lib/python3.6/site-packages/sklearn/base.py:334: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.19.1 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Create an object of Sonar Hate Speech Detection\n",
    "sonar = Sonar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hate_speech_classifier(text, Class):\n",
    "    for i in text:\n",
    "        sonar_dict = sonar.ping(text=i)\n",
    "        Class.append(list(sonar_dict.values())[1])\n",
    "        \n",
    "def sentiment_classifier(text):\n",
    "    sent = []\n",
    "    k = 1\n",
    "    for i in text:\n",
    "        result = sentimentanalyzer(text)[0]\n",
    "        print(str(k) + ' Done')\n",
    "        sent.append(result['label'])\n",
    "        k+=1\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentimentanalyzer = pipeline(\"sentiment-analysis\", device = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hate Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training data\n",
    "hate_speech_class_train = []\n",
    "hate_speech_classifier(text_train, hate_speech_class_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neither',\n",
       " 'neither',\n",
       " 'neither',\n",
       " 'offensive_language',\n",
       " 'neither',\n",
       " 'hate_speech',\n",
       " 'neither',\n",
       " 'neither',\n",
       " 'neither',\n",
       " 'neither']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hate_speech_class_train[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 = neither, 1 = hate_speech, 2 = offensive_language\n",
    "\n",
    "hate_speech_label_train = []\n",
    "\n",
    "for i in hate_speech_class_train:\n",
    "    if i == 'neither':\n",
    "        hate_speech_label_train.append(0)\n",
    "        \n",
    "    elif i == 'hate_speech':\n",
    "        hate_speech_label_train.append(1)\n",
    "        \n",
    "    elif i == 'offensive_language':\n",
    "        hate_speech_label_train.append(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 2, 0, 1, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hate_speech_label_train[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Done\n",
      "2 Done\n",
      "3 Done\n",
      "4 Done\n",
      "5 Done\n",
      "6 Done\n",
      "7 Done\n",
      "8 Done\n",
      "9 Done\n",
      "10 Done\n",
      "11 Done\n",
      "12 Done\n",
      "13 Done\n",
      "14 Done\n",
      "15 Done\n",
      "16 Done\n",
      "17 Done\n",
      "18 Done\n",
      "19 Done\n",
      "20 Done\n",
      "21 Done\n",
      "22 Done\n",
      "23 Done\n",
      "24 Done\n",
      "25 Done\n",
      "26 Done\n",
      "27 Done\n",
      "28 Done\n",
      "29 Done\n",
      "30 Done\n",
      "31 Done\n",
      "32 Done\n",
      "33 Done\n",
      "34 Done\n",
      "35 Done\n",
      "36 Done\n",
      "37 Done\n",
      "38 Done\n",
      "39 Done\n",
      "40 Done\n",
      "41 Done\n",
      "42 Done\n",
      "43 Done\n",
      "44 Done\n",
      "45 Done\n",
      "46 Done\n",
      "47 Done\n",
      "48 Done\n",
      "49 Done\n",
      "50 Done\n",
      "51 Done\n",
      "52 Done\n",
      "53 Done\n",
      "54 Done\n",
      "55 Done\n",
      "56 Done\n",
      "57 Done\n",
      "58 Done\n",
      "59 Done\n",
      "60 Done\n",
      "61 Done\n",
      "62 Done\n",
      "63 Done\n",
      "64 Done\n",
      "65 Done\n",
      "66 Done\n",
      "67 Done\n",
      "68 Done\n",
      "69 Done\n",
      "70 Done\n",
      "71 Done\n",
      "72 Done\n",
      "73 Done\n",
      "74 Done\n",
      "75 Done\n",
      "76 Done\n",
      "77 Done\n",
      "78 Done\n",
      "79 Done\n",
      "80 Done\n",
      "81 Done\n",
      "82 Done\n",
      "83 Done\n",
      "84 Done\n",
      "85 Done\n",
      "86 Done\n",
      "87 Done\n",
      "88 Done\n",
      "89 Done\n",
      "90 Done\n",
      "91 Done\n",
      "92 Done\n",
      "93 Done\n",
      "94 Done\n",
      "95 Done\n",
      "96 Done\n",
      "97 Done\n",
      "98 Done\n",
      "99 Done\n",
      "100 Done\n",
      "101 Done\n",
      "102 Done\n",
      "103 Done\n",
      "104 Done\n",
      "105 Done\n",
      "106 Done\n",
      "107 Done\n",
      "108 Done\n",
      "109 Done\n",
      "110 Done\n",
      "111 Done\n",
      "112 Done\n",
      "113 Done\n",
      "114 Done\n",
      "115 Done\n",
      "116 Done\n",
      "117 Done\n",
      "118 Done\n",
      "119 Done\n",
      "120 Done\n",
      "121 Done\n",
      "122 Done\n",
      "123 Done\n",
      "124 Done\n",
      "125 Done\n",
      "126 Done\n",
      "127 Done\n",
      "128 Done\n",
      "129 Done\n",
      "130 Done\n",
      "131 Done\n",
      "132 Done\n",
      "133 Done\n",
      "134 Done\n",
      "135 Done\n",
      "136 Done\n",
      "137 Done\n",
      "138 Done\n",
      "139 Done\n",
      "140 Done\n",
      "141 Done\n",
      "142 Done\n",
      "143 Done\n",
      "144 Done\n",
      "145 Done\n",
      "146 Done\n",
      "147 Done\n",
      "148 Done\n",
      "149 Done\n",
      "150 Done\n",
      "151 Done\n",
      "152 Done\n",
      "153 Done\n",
      "154 Done\n",
      "155 Done\n",
      "156 Done\n",
      "157 Done\n",
      "158 Done\n",
      "159 Done\n",
      "160 Done\n",
      "161 Done\n",
      "162 Done\n",
      "163 Done\n",
      "164 Done\n",
      "165 Done\n",
      "166 Done\n",
      "167 Done\n",
      "168 Done\n",
      "169 Done\n",
      "170 Done\n",
      "171 Done\n",
      "172 Done\n",
      "173 Done\n",
      "174 Done\n",
      "175 Done\n",
      "176 Done\n",
      "177 Done\n",
      "178 Done\n",
      "179 Done\n",
      "180 Done\n",
      "181 Done\n",
      "182 Done\n",
      "183 Done\n",
      "184 Done\n",
      "185 Done\n",
      "186 Done\n",
      "187 Done\n",
      "188 Done\n",
      "189 Done\n",
      "190 Done\n",
      "191 Done\n",
      "192 Done\n",
      "193 Done\n",
      "194 Done\n",
      "195 Done\n",
      "196 Done\n",
      "197 Done\n",
      "198 Done\n",
      "199 Done\n",
      "200 Done\n",
      "201 Done\n",
      "202 Done\n",
      "203 Done\n",
      "204 Done\n",
      "205 Done\n",
      "206 Done\n",
      "207 Done\n",
      "208 Done\n",
      "209 Done\n",
      "210 Done\n",
      "211 Done\n",
      "212 Done\n",
      "213 Done\n",
      "214 Done\n",
      "215 Done\n",
      "216 Done\n",
      "217 Done\n",
      "218 Done\n",
      "219 Done\n",
      "220 Done\n",
      "221 Done\n",
      "222 Done\n",
      "223 Done\n",
      "224 Done\n",
      "225 Done\n",
      "226 Done\n",
      "227 Done\n",
      "228 Done\n",
      "229 Done\n",
      "230 Done\n",
      "231 Done\n",
      "232 Done\n",
      "233 Done\n",
      "234 Done\n",
      "235 Done\n",
      "236 Done\n",
      "237 Done\n",
      "238 Done\n",
      "239 Done\n",
      "240 Done\n",
      "241 Done\n",
      "242 Done\n",
      "243 Done\n",
      "244 Done\n",
      "245 Done\n",
      "246 Done\n",
      "247 Done\n",
      "248 Done\n",
      "249 Done\n",
      "250 Done\n",
      "251 Done\n",
      "252 Done\n",
      "253 Done\n",
      "254 Done\n",
      "255 Done\n",
      "256 Done\n",
      "257 Done\n",
      "258 Done\n",
      "259 Done\n",
      "260 Done\n",
      "261 Done\n",
      "262 Done\n",
      "263 Done\n",
      "264 Done\n",
      "265 Done\n",
      "266 Done\n",
      "267 Done\n",
      "268 Done\n",
      "269 Done\n",
      "270 Done\n",
      "271 Done\n",
      "272 Done\n",
      "273 Done\n",
      "274 Done\n",
      "275 Done\n",
      "276 Done\n",
      "277 Done\n",
      "278 Done\n",
      "279 Done\n",
      "280 Done\n",
      "281 Done\n",
      "282 Done\n",
      "283 Done\n",
      "284 Done\n",
      "285 Done\n",
      "286 Done\n",
      "287 Done\n",
      "288 Done\n",
      "289 Done\n",
      "290 Done\n",
      "291 Done\n",
      "292 Done\n",
      "293 Done\n",
      "294 Done\n",
      "295 Done\n",
      "296 Done\n",
      "297 Done\n",
      "298 Done\n",
      "299 Done\n",
      "300 Done\n",
      "301 Done\n",
      "302 Done\n",
      "303 Done\n",
      "304 Done\n",
      "305 Done\n",
      "306 Done\n",
      "307 Done\n",
      "308 Done\n",
      "309 Done\n",
      "310 Done\n",
      "311 Done\n",
      "312 Done\n",
      "313 Done\n",
      "314 Done\n",
      "315 Done\n",
      "316 Done\n",
      "317 Done\n",
      "318 Done\n",
      "319 Done\n",
      "320 Done\n",
      "321 Done\n",
      "322 Done\n",
      "323 Done\n",
      "324 Done\n",
      "325 Done\n",
      "326 Done\n",
      "327 Done\n",
      "328 Done\n",
      "329 Done\n",
      "330 Done\n",
      "331 Done\n",
      "332 Done\n",
      "333 Done\n",
      "334 Done\n",
      "335 Done\n",
      "336 Done\n",
      "337 Done\n",
      "338 Done\n",
      "339 Done\n",
      "340 Done\n",
      "341 Done\n",
      "342 Done\n",
      "343 Done\n",
      "344 Done\n",
      "345 Done\n",
      "346 Done\n",
      "347 Done\n",
      "348 Done\n",
      "349 Done\n",
      "350 Done\n",
      "351 Done\n",
      "352 Done\n",
      "353 Done\n",
      "354 Done\n",
      "355 Done\n",
      "356 Done\n",
      "357 Done\n",
      "358 Done\n",
      "359 Done\n",
      "360 Done\n",
      "361 Done\n",
      "362 Done\n",
      "363 Done\n",
      "364 Done\n",
      "365 Done\n",
      "366 Done\n",
      "367 Done\n",
      "368 Done\n",
      "369 Done\n",
      "370 Done\n",
      "371 Done\n",
      "372 Done\n",
      "373 Done\n",
      "374 Done\n",
      "375 Done\n",
      "376 Done\n",
      "377 Done\n",
      "378 Done\n",
      "379 Done\n",
      "380 Done\n",
      "381 Done\n",
      "382 Done\n",
      "383 Done\n",
      "384 Done\n",
      "385 Done\n",
      "386 Done\n",
      "387 Done\n",
      "388 Done\n",
      "389 Done\n",
      "390 Done\n",
      "391 Done\n",
      "392 Done\n",
      "393 Done\n",
      "394 Done\n",
      "395 Done\n",
      "396 Done\n",
      "397 Done\n",
      "398 Done\n",
      "399 Done\n",
      "400 Done\n",
      "401 Done\n",
      "402 Done\n",
      "403 Done\n",
      "404 Done\n",
      "405 Done\n",
      "406 Done\n",
      "407 Done\n",
      "408 Done\n",
      "409 Done\n",
      "410 Done\n",
      "411 Done\n",
      "412 Done\n",
      "413 Done\n",
      "414 Done\n",
      "415 Done\n",
      "416 Done\n",
      "417 Done\n",
      "418 Done\n",
      "419 Done\n",
      "420 Done\n",
      "421 Done\n",
      "422 Done\n",
      "423 Done\n",
      "424 Done\n",
      "425 Done\n",
      "426 Done\n",
      "427 Done\n",
      "428 Done\n",
      "429 Done\n",
      "430 Done\n",
      "431 Done\n",
      "432 Done\n",
      "433 Done\n",
      "434 Done\n",
      "435 Done\n",
      "436 Done\n",
      "437 Done\n",
      "438 Done\n",
      "439 Done\n",
      "440 Done\n",
      "441 Done\n",
      "442 Done\n",
      "443 Done\n",
      "444 Done\n",
      "445 Done\n",
      "446 Done\n",
      "447 Done\n",
      "448 Done\n",
      "449 Done\n",
      "450 Done\n",
      "451 Done\n",
      "452 Done\n",
      "453 Done\n",
      "454 Done\n",
      "455 Done\n",
      "456 Done\n",
      "457 Done\n",
      "458 Done\n",
      "459 Done\n",
      "460 Done\n",
      "461 Done\n",
      "462 Done\n",
      "463 Done\n",
      "464 Done\n",
      "465 Done\n",
      "466 Done\n",
      "467 Done\n",
      "468 Done\n",
      "469 Done\n",
      "470 Done\n",
      "471 Done\n",
      "472 Done\n",
      "473 Done\n",
      "474 Done\n",
      "475 Done\n",
      "476 Done\n",
      "477 Done\n",
      "478 Done\n",
      "479 Done\n",
      "480 Done\n",
      "481 Done\n",
      "482 Done\n",
      "483 Done\n",
      "484 Done\n",
      "485 Done\n",
      "486 Done\n",
      "487 Done\n",
      "488 Done\n",
      "489 Done\n",
      "490 Done\n",
      "491 Done\n",
      "492 Done\n",
      "493 Done\n",
      "494 Done\n",
      "495 Done\n",
      "496 Done\n",
      "497 Done\n",
      "498 Done\n",
      "499 Done\n",
      "500 Done\n",
      "501 Done\n",
      "502 Done\n",
      "503 Done\n",
      "504 Done\n",
      "505 Done\n",
      "506 Done\n",
      "507 Done\n",
      "508 Done\n",
      "509 Done\n",
      "510 Done\n",
      "511 Done\n",
      "512 Done\n",
      "513 Done\n",
      "514 Done\n",
      "515 Done\n",
      "516 Done\n",
      "517 Done\n",
      "518 Done\n",
      "519 Done\n",
      "520 Done\n",
      "521 Done\n",
      "522 Done\n",
      "523 Done\n",
      "524 Done\n",
      "525 Done\n",
      "526 Done\n",
      "527 Done\n",
      "528 Done\n",
      "529 Done\n",
      "530 Done\n",
      "531 Done\n",
      "532 Done\n",
      "533 Done\n",
      "534 Done\n",
      "535 Done\n",
      "536 Done\n",
      "537 Done\n",
      "538 Done\n",
      "539 Done\n",
      "540 Done\n",
      "541 Done\n",
      "542 Done\n",
      "543 Done\n",
      "544 Done\n",
      "545 Done\n",
      "546 Done\n",
      "547 Done\n",
      "548 Done\n",
      "549 Done\n",
      "550 Done\n",
      "551 Done\n",
      "552 Done\n",
      "553 Done\n",
      "554 Done\n",
      "555 Done\n",
      "556 Done\n",
      "557 Done\n",
      "558 Done\n",
      "559 Done\n",
      "560 Done\n",
      "561 Done\n",
      "562 Done\n",
      "563 Done\n",
      "564 Done\n",
      "565 Done\n",
      "566 Done\n",
      "567 Done\n",
      "568 Done\n",
      "569 Done\n",
      "570 Done\n",
      "571 Done\n",
      "572 Done\n",
      "573 Done\n",
      "574 Done\n",
      "575 Done\n",
      "576 Done\n",
      "577 Done\n",
      "578 Done\n",
      "579 Done\n",
      "580 Done\n",
      "581 Done\n",
      "582 Done\n",
      "583 Done\n",
      "584 Done\n",
      "585 Done\n",
      "586 Done\n",
      "587 Done\n",
      "588 Done\n",
      "589 Done\n",
      "590 Done\n",
      "591 Done\n",
      "592 Done\n",
      "593 Done\n",
      "594 Done\n",
      "595 Done\n",
      "596 Done\n",
      "597 Done\n",
      "598 Done\n",
      "599 Done\n",
      "600 Done\n",
      "601 Done\n",
      "602 Done\n",
      "603 Done\n",
      "604 Done\n",
      "605 Done\n",
      "606 Done\n",
      "607 Done\n",
      "608 Done\n",
      "609 Done\n",
      "610 Done\n",
      "611 Done\n",
      "612 Done\n",
      "613 Done\n",
      "614 Done\n",
      "615 Done\n",
      "616 Done\n",
      "617 Done\n",
      "618 Done\n",
      "619 Done\n",
      "620 Done\n",
      "621 Done\n",
      "622 Done\n",
      "623 Done\n",
      "624 Done\n",
      "625 Done\n",
      "626 Done\n",
      "627 Done\n",
      "628 Done\n",
      "629 Done\n",
      "630 Done\n",
      "631 Done\n",
      "632 Done\n",
      "633 Done\n",
      "634 Done\n",
      "635 Done\n",
      "636 Done\n",
      "637 Done\n",
      "638 Done\n",
      "639 Done\n",
      "640 Done\n",
      "641 Done\n",
      "642 Done\n",
      "643 Done\n",
      "644 Done\n",
      "645 Done\n",
      "646 Done\n",
      "647 Done\n",
      "648 Done\n",
      "649 Done\n",
      "650 Done\n",
      "651 Done\n",
      "652 Done\n",
      "653 Done\n",
      "654 Done\n",
      "655 Done\n",
      "656 Done\n",
      "657 Done\n",
      "658 Done\n",
      "659 Done\n",
      "660 Done\n",
      "661 Done\n",
      "662 Done\n",
      "663 Done\n",
      "664 Done\n",
      "665 Done\n",
      "666 Done\n",
      "667 Done\n",
      "668 Done\n",
      "669 Done\n",
      "670 Done\n",
      "671 Done\n",
      "672 Done\n",
      "673 Done\n",
      "674 Done\n",
      "675 Done\n",
      "676 Done\n",
      "677 Done\n",
      "678 Done\n",
      "679 Done\n",
      "680 Done\n",
      "681 Done\n",
      "682 Done\n",
      "683 Done\n",
      "684 Done\n",
      "685 Done\n",
      "686 Done\n",
      "687 Done\n",
      "688 Done\n",
      "689 Done\n",
      "690 Done\n",
      "691 Done\n",
      "692 Done\n",
      "693 Done\n",
      "694 Done\n",
      "695 Done\n",
      "696 Done\n",
      "697 Done\n",
      "698 Done\n",
      "699 Done\n",
      "700 Done\n",
      "701 Done\n",
      "702 Done\n",
      "703 Done\n",
      "704 Done\n",
      "705 Done\n",
      "706 Done\n",
      "707 Done\n",
      "708 Done\n",
      "709 Done\n",
      "710 Done\n",
      "711 Done\n",
      "712 Done\n",
      "713 Done\n",
      "714 Done\n",
      "715 Done\n",
      "716 Done\n",
      "717 Done\n",
      "718 Done\n",
      "719 Done\n",
      "720 Done\n",
      "721 Done\n",
      "722 Done\n",
      "723 Done\n",
      "724 Done\n",
      "725 Done\n",
      "726 Done\n",
      "727 Done\n"
     ]
    }
   ],
   "source": [
    "sentiment_train = sentiment_classifier(text_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NEGATIVE'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(sentiment_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All sentiment is negative. Since there is no variation, the models will only learn negative. This won't add anything to modeling. This will be left out in model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hate Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training data\n",
    "hate_speech_class_test = []\n",
    "hate_speech_classifier(text_test, hate_speech_class_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neither',\n",
       " 'neither',\n",
       " 'neither',\n",
       " 'neither',\n",
       " 'neither',\n",
       " 'neither',\n",
       " 'neither',\n",
       " 'neither',\n",
       " 'neither',\n",
       " 'offensive_language']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hate_speech_class_test[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 = neither, 1 = hate_speech, 2 = offensive_language\n",
    "\n",
    "hate_speech_label_test = []\n",
    "\n",
    "for i in hate_speech_class_test:\n",
    "    if i == 'neither':\n",
    "        hate_speech_label_test.append(0)\n",
    "        \n",
    "    elif i == 'hate_speech':\n",
    "        hate_speech_label_test.append(1)\n",
    "        \n",
    "    elif i == 'offensive_language':\n",
    "        hate_speech_label_test.append(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 2]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hate_speech_label_test[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assemble DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df = pd.DataFrame({'Image' : image_name_train,\n",
    "                            'Text' : text_train,\n",
    "                            'Hate' : hate_speech_label_train,\n",
    "                            'Labels' : one_hot_labels_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>Text</th>\n",
       "      <th>Hate</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>182_image.png</td>\n",
       "      <td>The most costly errors in all of history\\n\\nWo...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>366_image_batch_2.png</td>\n",
       "      <td>MY PARENTS WERE KILLED AS A RESULT OF A GUN FR...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>148_image.png</td>\n",
       "      <td>VOTE IT OUT\\nVOTE IT OUT\\n\\nHE SAID IT WASN'T ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58_image_batch_2.png</td>\n",
       "      <td>IF YOU DONT LISTEN TO DESPACITO YOU AIN'T LATI...</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>816_image_batch_2.png</td>\n",
       "      <td>FAIR AND BALANCED\\n\\nFAUX NEWS\\n\\nWE DISTORT Y...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>722</th>\n",
       "      <td>687_image_batch_2.png</td>\n",
       "      <td>Asked what the \"D.C.\" stands for in \"Washingto...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723</th>\n",
       "      <td>516_image_batch_2.png</td>\n",
       "      <td>BREAKING NEWS: The Chicago Police Dept has rep...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724</th>\n",
       "      <td>45_image_batch_2.png</td>\n",
       "      <td>IF WE GIVE UP EVERYTHING THAT OFFENDS SOMEONE ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725</th>\n",
       "      <td>86_image_batch_2.png</td>\n",
       "      <td>YOU'D THINK YOU WERE IN A WHORE HOUSE, SEEING ...</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726</th>\n",
       "      <td>636_image_batch_2.png</td>\n",
       "      <td>Naturopathy,\\nFor those who want to look like ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>727 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Image                                               Text  \\\n",
       "0            182_image.png  The most costly errors in all of history\\n\\nWo...   \n",
       "1    366_image_batch_2.png  MY PARENTS WERE KILLED AS A RESULT OF A GUN FR...   \n",
       "2            148_image.png  VOTE IT OUT\\nVOTE IT OUT\\n\\nHE SAID IT WASN'T ...   \n",
       "3     58_image_batch_2.png  IF YOU DONT LISTEN TO DESPACITO YOU AIN'T LATI...   \n",
       "4    816_image_batch_2.png  FAIR AND BALANCED\\n\\nFAUX NEWS\\n\\nWE DISTORT Y...   \n",
       "..                     ...                                                ...   \n",
       "722  687_image_batch_2.png  Asked what the \"D.C.\" stands for in \"Washingto...   \n",
       "723  516_image_batch_2.png  BREAKING NEWS: The Chicago Police Dept has rep...   \n",
       "724   45_image_batch_2.png  IF WE GIVE UP EVERYTHING THAT OFFENDS SOMEONE ...   \n",
       "725   86_image_batch_2.png  YOU'D THINK YOU WERE IN A WHORE HOUSE, SEEING ...   \n",
       "726  636_image_batch_2.png  Naturopathy,\\nFor those who want to look like ...   \n",
       "\n",
       "     Hate                                             Labels  \n",
       "0       0  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, ...  \n",
       "1       0  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, ...  \n",
       "2       0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...  \n",
       "3       2  [0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4       0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...  \n",
       "..    ...                                                ...  \n",
       "722     0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "723     0  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "724     0  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "725     2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, ...  \n",
       "726     0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, ...  \n",
       "\n",
       "[727 rows x 4 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame({'Image' : image_name_test,\n",
    "                            'Text' : text_test,\n",
    "                            'Hate' : hate_speech_label_test,\n",
    "                            'Labels' : one_hot_labels_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>Text</th>\n",
       "      <th>Hate</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>566_image_batch_2.png</td>\n",
       "      <td>A FRIENDLY REMINDER...\\n\\nFERGUSON\\n\\nBALTIMOR...</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>738_image_batch_2.png</td>\n",
       "      <td>We're fed up.\\nWe're not asking for our countr...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>885_image_batch_2.png</td>\n",
       "      <td>PRESIDENTS' DAY SALE\\n\\nEVERYONE MUST GO</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41_image_batch_2.png</td>\n",
       "      <td>TRUDEAU'S PRIORITIES\\n1.ENRICHING HIS FRIENDS ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>169_image.png</td>\n",
       "      <td>Our elders were called to war to save lives.\\n...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>867_image_batch_2.png</td>\n",
       "      <td>AT THIS POINT, SHOULDN'T HIS RALLIES BE CONSID...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>901_image_batch_2.png</td>\n",
       "      <td>HEY YOU GUYSSSS!\\n\\nI CAN TRAVEL NOW\\nTHE VACC...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>932_image_batch_2.png</td>\n",
       "      <td>President Trump, infected with COVID-19, retur...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>967_image_batch_2.png</td>\n",
       "      <td>Your parades are cute!\\n\\nWait till you see ou...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>171_image_batch_2.png</td>\n",
       "      <td>Americans when Newton discovered gravity :\\n\\n...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>187 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Image                                               Text  \\\n",
       "0    566_image_batch_2.png  A FRIENDLY REMINDER...\\n\\nFERGUSON\\n\\nBALTIMOR...   \n",
       "1    738_image_batch_2.png  We're fed up.\\nWe're not asking for our countr...   \n",
       "2    885_image_batch_2.png           PRESIDENTS' DAY SALE\\n\\nEVERYONE MUST GO   \n",
       "3     41_image_batch_2.png  TRUDEAU'S PRIORITIES\\n1.ENRICHING HIS FRIENDS ...   \n",
       "4            169_image.png  Our elders were called to war to save lives.\\n...   \n",
       "..                     ...                                                ...   \n",
       "182  867_image_batch_2.png  AT THIS POINT, SHOULDN'T HIS RALLIES BE CONSID...   \n",
       "183  901_image_batch_2.png  HEY YOU GUYSSSS!\\n\\nI CAN TRAVEL NOW\\nTHE VACC...   \n",
       "184  932_image_batch_2.png  President Trump, infected with COVID-19, retur...   \n",
       "185  967_image_batch_2.png  Your parades are cute!\\n\\nWait till you see ou...   \n",
       "186  171_image_batch_2.png  Americans when Newton discovered gravity :\\n\\n...   \n",
       "\n",
       "     Hate                                             Labels  \n",
       "0       0  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1       0  [0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, ...  \n",
       "2       0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3       0  [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, ...  \n",
       "4       0  [0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "..    ...                                                ...  \n",
       "182     0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...  \n",
       "183     0  [0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "184     0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, ...  \n",
       "185     0  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, ...  \n",
       "186     0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, ...  \n",
       "\n",
       "[187 rows x 4 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df.to_json('MultiModal_training_data.json')\n",
    "test_df.to_json('MultiModal_testing_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>Text</th>\n",
       "      <th>Hate</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>182_image.png</td>\n",
       "      <td>The most costly errors in all of history\\n\\nWo...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>366_image_batch_2.png</td>\n",
       "      <td>MY PARENTS WERE KILLED AS A RESULT OF A GUN FR...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>148_image.png</td>\n",
       "      <td>VOTE IT OUT\\nVOTE IT OUT\\n\\nHE SAID IT WASN'T ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58_image_batch_2.png</td>\n",
       "      <td>IF YOU DONT LISTEN TO DESPACITO YOU AIN'T LATI...</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>816_image_batch_2.png</td>\n",
       "      <td>FAIR AND BALANCED\\n\\nFAUX NEWS\\n\\nWE DISTORT Y...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>722</th>\n",
       "      <td>687_image_batch_2.png</td>\n",
       "      <td>Asked what the \"D.C.\" stands for in \"Washingto...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723</th>\n",
       "      <td>516_image_batch_2.png</td>\n",
       "      <td>BREAKING NEWS: The Chicago Police Dept has rep...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724</th>\n",
       "      <td>45_image_batch_2.png</td>\n",
       "      <td>IF WE GIVE UP EVERYTHING THAT OFFENDS SOMEONE ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725</th>\n",
       "      <td>86_image_batch_2.png</td>\n",
       "      <td>YOU'D THINK YOU WERE IN A WHORE HOUSE, SEEING ...</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726</th>\n",
       "      <td>636_image_batch_2.png</td>\n",
       "      <td>Naturopathy,\\nFor those who want to look like ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>727 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Image                                               Text  \\\n",
       "0            182_image.png  The most costly errors in all of history\\n\\nWo...   \n",
       "1    366_image_batch_2.png  MY PARENTS WERE KILLED AS A RESULT OF A GUN FR...   \n",
       "2            148_image.png  VOTE IT OUT\\nVOTE IT OUT\\n\\nHE SAID IT WASN'T ...   \n",
       "3     58_image_batch_2.png  IF YOU DONT LISTEN TO DESPACITO YOU AIN'T LATI...   \n",
       "4    816_image_batch_2.png  FAIR AND BALANCED\\n\\nFAUX NEWS\\n\\nWE DISTORT Y...   \n",
       "..                     ...                                                ...   \n",
       "722  687_image_batch_2.png  Asked what the \"D.C.\" stands for in \"Washingto...   \n",
       "723  516_image_batch_2.png  BREAKING NEWS: The Chicago Police Dept has rep...   \n",
       "724   45_image_batch_2.png  IF WE GIVE UP EVERYTHING THAT OFFENDS SOMEONE ...   \n",
       "725   86_image_batch_2.png  YOU'D THINK YOU WERE IN A WHORE HOUSE, SEEING ...   \n",
       "726  636_image_batch_2.png  Naturopathy,\\nFor those who want to look like ...   \n",
       "\n",
       "     Hate                                             Labels  \n",
       "0       0  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, ...  \n",
       "1       0  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, ...  \n",
       "2       0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...  \n",
       "3       2  [0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4       0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...  \n",
       "..    ...                                                ...  \n",
       "722     0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "723     0  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "724     0  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "725     2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, ...  \n",
       "726     0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, ...  \n",
       "\n",
       "[727 rows x 4 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_json('MultiModal_training_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 727 entries, 0 to 726\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Image   727 non-null    object\n",
      " 1   Text    727 non-null    object\n",
      " 2   Hate    727 non-null    int64 \n",
      " 3   Labels  727 non-null    object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 28.4+ KB\n"
     ]
    }
   ],
   "source": [
    "pd.read_json('MultiModal_training_data.json').info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
