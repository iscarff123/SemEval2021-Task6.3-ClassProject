{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling - Treating Labels as Independent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "#%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import time\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from sklearn.model_selection import KFold\n",
    "import skimage\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in reference data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load in reference data\n",
    "\n",
    "with open('training_data_task3.txt') as f: ### Training data\n",
    "    training = json.load(f)\n",
    "    \n",
    "with open('testing_data_task3.txt') as f: ### Testing data\n",
    "    testing = json.load(f)\n",
    "    \n",
    "classes = ['Smears', 'Loaded Language', 'Name calling/Labeling', 'Glittering generalities (Virtue)',\n",
    "               'Appeal to (Strong) Emotions', 'Appeal to fear/prejudice', 'Transfer', 'Doubt',\n",
    "               'Exaggeration/Minimisation', 'Whataboutism', 'Slogans', 'Flag-waving',\n",
    "               \"Misrepresentation of Someone's Position (Straw Man)\", 'Causal Oversimplification',\n",
    "               'Thought-terminating clich√©', 'Black-and-white Fallacy/Dictatorship', 'Appeal to authority',\n",
    "               'Reductio ad hitlerum', 'Repetition', 'Obfuscation, Intentional vagueness, Confusion',\n",
    "               'Presenting Irrelevant Data (Red Herring)', 'Bandwagon']\n",
    "\n",
    "### Create Class Binarizer\n",
    "one_hot = MultiLabelBinarizer()\n",
    "one_hot.fit([classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(training) + len(testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Image Names & Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name = []\n",
    "one_hot_labels = [] ### List to hold one hot labels\n",
    "\n",
    "for obsv in training: ### Go through the training data\n",
    "\n",
    "    ### Images ###\n",
    "    image_name.append(obsv['image'])\n",
    "    \n",
    "    ### Labels\n",
    "    one_hot_labels.append(one_hot.transform([obsv['labels']])[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name2 = []\n",
    "one_hot_labels2 = [] ### List to hold one hot labels\n",
    "\n",
    "for obsv in testing: ### Go through the training data\n",
    "\n",
    "    ### Images ###\n",
    "    image_name2.append(obsv['image'])\n",
    "    \n",
    "    ### Labels\n",
    "    one_hot_labels2.append(one_hot.transform([obsv['labels']])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df = pd.DataFrame({\n",
    "    'images' : image_name,\n",
    "    'labels' : one_hot_labels\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_df = pd.DataFrame({\n",
    "    'images' : image_name2,\n",
    "    'labels' : one_hot_labels2\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save to csv\n",
    "training_df.to_json('training_data.json')\n",
    "testing_df.to_json('testing_data.json')\n",
    "### Saving as json retains list type in dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "# \n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data with DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CustomLoader import ImageLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = ImageLoader(json_file = 'training_data.json', root_dir = 'Images',\n",
    "                           transform = transforms.Compose([\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Resize(size = (224,224)),\n",
    "                               transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)) ### Pixel range [-1,1]\n",
    "                           ]))\n",
    "\n",
    "testing_data = ImageLoader(json_file = 'testing_data.json', root_dir = 'Images',\n",
    "                           transform = transforms.Compose([\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Resize(size = (224,224)),\n",
    "                               transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)) ### Pixel range [-1,1]\n",
    "                           ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset = training_data, batch_size = 25, shuffle = True)\n",
    "test_loader = DataLoader(dataset = testing_data, batch_size = 25, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5\n",
    "    plt.imshow(np.transpose(img, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_batch = iter(train_loader)\n",
    "images_batch, labels_batch = first_batch.next()\n",
    "images_batch = images_batch.numpy()\n",
    "labels_batch = labels_batch.numpy()\n",
    "\n",
    "fig = plt.figure(figsize = (25, 15))\n",
    "\n",
    "for idx in np.arange(25):\n",
    "    ax = fig.add_subplot(5, 5, idx + 1)\n",
    "    imshow(images_batch[idx])\n",
    "    \n",
    "    title = \"\"\n",
    "    k = 1\n",
    "    for i in np.where(labels_batch[idx] == 1)[0]:\n",
    "        if k == 1:\n",
    "            title = title + one_hot.classes_[i]\n",
    "            k+=1\n",
    "        else:\n",
    "            title = title + '\\n' + one_hot.classes_[i]\n",
    "            \n",
    "    ax.set_title(title)\n",
    "        \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_batch = iter(test_loader)\n",
    "images_batch, labels_batch = first_batch.next()\n",
    "images_batch = images_batch.numpy()\n",
    "labels_batch = labels_batch.numpy()\n",
    "\n",
    "fig = plt.figure(figsize = (25, 15))\n",
    "\n",
    "for idx in np.arange(25):\n",
    "    ax = fig.add_subplot(5, 5, idx + 1)\n",
    "    imshow(images_batch[idx])\n",
    "    \n",
    "    title = \"\"\n",
    "    k = 1\n",
    "    for i in np.where(labels_batch[idx] == 1)[0]:\n",
    "        if k == 1:\n",
    "            title = title + one_hot.classes_[i]\n",
    "            k+=1\n",
    "        else:\n",
    "            title = title + '\\n' + one_hot.classes_[i]\n",
    "            \n",
    "    ax.set_title(title)\n",
    "        \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "# \n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training Methodology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(optim):\n",
    "    for param_group in optim.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This function is used to train a cnn model\n",
    "def Kfold_train_CNN(model = None, training_data = None, learning_rate = None, k_folds = None, n_epochs = None, model_name = None):\n",
    "    \n",
    "    ### Check that all entries are valid\n",
    "    if ((model == None) or (training_data == None) or (model_name == None) or \n",
    "        (learning_rate == None) or (k_folds == None) or (n_epochs == None)):\n",
    "        print ('Enter all info.')\n",
    "        \n",
    "        \n",
    "        \n",
    "    ### Run K-Fold CV\n",
    "    else:\n",
    "        \n",
    "        device = 'cpu'\n",
    "\n",
    "        ### Set Loss Function and Optimizer\n",
    "        criterion = nn.BCELoss()\n",
    "        \n",
    "        \n",
    "        #### Define the K-fold Cross Validator\n",
    "        kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "        ### Create values to hold the best model metrics across folds\n",
    "        val_loss_min = np.Inf ### This determines best model\n",
    "        \n",
    "        best_train_loss = 0\n",
    "        best_train_acc = 0\n",
    "        best_train_f1_mic = 0\n",
    "        best_train_f1_mac = 0\n",
    "        best_train_prec_mic = 0\n",
    "        best_train_prec_mac = 0\n",
    "        best_train_rec_mic = 0\n",
    "        best_train_rec_mac = 0\n",
    "\n",
    "        best_val_acc = 0\n",
    "        best_val_f1_mic = 0\n",
    "        best_val_f1_mac = 0\n",
    "        best_val_prec_mic = 0\n",
    "        best_val_prec_mac = 0\n",
    "        best_val_rec_mic = 0\n",
    "        best_val_rec_mac = 0\n",
    "        \n",
    "        best_fold = 0\n",
    "        best_epoch = 0\n",
    "        \n",
    "\n",
    "        \n",
    "        ### Start print\n",
    "\n",
    "        start = time.time()\n",
    "        \n",
    "        ### K-fold Cross Validation model evaluation\n",
    "        for fold, (train_ids, val_ids) in enumerate(kfold.split(training_data)):\n",
    "            \n",
    "            print('-------------------------------------------')\n",
    "            print('FOLD {}'.format(fold))\n",
    "            print('-------------------------------------------')\n",
    "            \n",
    "            ### Sample elements randomly from a given list of ids, no replacement\n",
    "            train_subsampler = SubsetRandomSampler(train_ids)\n",
    "            val_subsampler = SubsetRandomSampler(val_ids)\n",
    "            \n",
    "            ### Define data loaders for training and validation in current fold\n",
    "            train_loader = DataLoader(dataset = training_data, batch_size = 25, sampler = train_subsampler)\n",
    "            val_loader = DataLoader(dataset = training_data, batch_size = 25, sampler = val_subsampler)\n",
    "            \n",
    "            ### Initialize network\n",
    "            network = model\n",
    "            if torch.cuda.is_available():\n",
    "                network.cuda()\n",
    "                network = nn.DataParallel(network, list(range(2)))\n",
    "                device = 'cuda'\n",
    "            \n",
    "            ### Initialize optimizer\n",
    "            optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "            scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode = 'min', factor = 0.1, patience = 5)\n",
    "            \n",
    "            \n",
    "            ### Create lists of values at the end of each epoch for each fold\n",
    "            train_loss = []\n",
    "            train_acc = []\n",
    "            train_f1_mic = []\n",
    "            train_f1_mac = []\n",
    "            train_prec_mic = []\n",
    "            train_prec_mac = []\n",
    "            train_rec_mic = []\n",
    "            train_rec_mac = []\n",
    "            \n",
    "            val_loss = []\n",
    "            val_acc = []\n",
    "            val_f1_mic = []\n",
    "            val_f1_mac = []\n",
    "            val_prec_mic = []\n",
    "            val_prec_mac = []\n",
    "            val_rec_mic = []\n",
    "            val_rec_mac = []\n",
    "            \n",
    "            \n",
    "            ### Train network\n",
    "            for epoch in range(n_epochs):\n",
    "                \n",
    "                ### Hold training predictions and targets\n",
    "                train_output = np.empty((0,22), int)\n",
    "                train_all_targets = np.empty((0,22), int)\n",
    "                \n",
    "                val_output = np.empty((0,22), int)\n",
    "                val_all_targets = np.empty((0,22), int)\n",
    "                \n",
    "                \n",
    "                ### Train ###\n",
    "                network.train()\n",
    "                \n",
    "                train_running_loss = 0.0\n",
    "                \n",
    "                batch_number = 0\n",
    "                for i, data in enumerate(train_loader):\n",
    "                    \n",
    "                    images, targets = data[0].to(device), data[1].float().to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    output = network(images)\n",
    "                    \n",
    "                    loss = criterion(output, targets) \n",
    "                    train_running_loss += loss.item()\n",
    "                                        \n",
    "                    ### Append output\n",
    "                    train_output = np.vstack((train_output, ((output > 0.5).cpu().numpy().astype('int'))))\n",
    "                    train_all_targets = np.vstack((train_all_targets, targets.cpu().numpy().astype('int')))\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                                        \n",
    "                    \n",
    "                ### Calculate metrics and append\n",
    "                train_loss.append(train_running_loss/len(train_loader.dataset))\n",
    "                train_acc.append(accuracy_score(train_all_targets, train_output))\n",
    "                train_f1_mic.append(f1_score(train_all_targets, train_output, average = 'micro'))\n",
    "                train_f1_mac.append(f1_score(train_all_targets, train_output, average = 'macro'))\n",
    "                train_prec_mic.append(precision_score(train_all_targets, train_output, average = 'micro'))\n",
    "                train_prec_mac.append(precision_score(train_all_targets, train_output, average = 'macro'))\n",
    "                train_rec_mic.append(recall_score(train_all_targets, train_output, average = 'micro'))\n",
    "                train_rec_mac.append(recall_score(train_all_targets, train_output, average = 'macro'))\n",
    "                \n",
    "                \n",
    "                ### Validate###\n",
    "                network.eval()\n",
    "                val_running_loss = 0.0\n",
    "\n",
    "                                \n",
    "                for i, data in enumerate(val_loader):\n",
    "                    images, targets = data[0].to(device), data[1].float().to(device)\n",
    "                    output = network(images)\n",
    "                    loss = criterion(output, targets)\n",
    "                    val_running_loss += loss.item()\n",
    "                    \n",
    "                    ### Append output\n",
    "                    val_output = np.vstack((val_output, ((output > 0.5).cpu().numpy().astype('int'))))\n",
    "                    val_all_targets = np.vstack((val_all_targets, targets.cpu().numpy().astype('int')))\n",
    "\n",
    "                \n",
    "                ### Calculate metrics and append\n",
    "                val_loss.append(val_running_loss/len(val_loader.dataset))\n",
    "                val_acc.append(accuracy_score(val_all_targets, val_output))\n",
    "                val_f1_mic.append(f1_score(val_all_targets, val_output, average = 'micro'))\n",
    "                val_f1_mac.append(f1_score(val_all_targets, val_output, average = 'macro'))\n",
    "                val_prec_mic.append(precision_score(val_all_targets, val_output, average = 'micro'))\n",
    "                val_prec_mac.append(precision_score(val_all_targets, val_output, average = 'macro'))\n",
    "                val_rec_mic.append(recall_score(val_all_targets, val_output, average = 'micro'))\n",
    "                val_rec_mac.append(recall_score(val_all_targets, val_output, average = 'macro'))\n",
    "                \n",
    "                \n",
    "                ### Save model with the lowest validation loss\n",
    "                if val_loss[epoch] <= val_loss_min:\n",
    "                    print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...\\n'.format(\n",
    "                    val_loss_min,\n",
    "                    val_loss[epoch]))\n",
    "                    torch.save(network.state_dict(), 'BestModels/' + model_name +'.pt')\n",
    "                    val_loss_min = val_loss[epoch]\n",
    "                    \n",
    "                    ### Set current best metrics\n",
    "                    best_train_loss = train_loss[epoch]\n",
    "                    best_train_acc = train_acc[epoch]\n",
    "                    best_train_f1_mic = train_f1_mic[epoch]\n",
    "                    best_train_f1_mac = train_f1_mac[epoch]\n",
    "                    best_train_prec_mic = train_prec_mic[epoch]\n",
    "                    best_train_prec_mac = train_prec_mac[epoch]\n",
    "                    best_train_rec_mic = train_rec_mic[epoch]\n",
    "                    best_train_rec_mac = train_rec_mac[epoch]\n",
    "\n",
    "                    best_val_acc = val_acc[epoch]\n",
    "                    best_val_f1_mic = val_f1_mic[epoch]\n",
    "                    best_val_f1_mac = val_f1_mac[epoch]\n",
    "                    best_val_prec_mic = val_prec_mic[epoch]\n",
    "                    best_val_prec_mac = val_prec_mac[epoch]\n",
    "                    best_val_rec_mic = val_rec_mic[epoch]\n",
    "                    best_val_rec_mac = val_rec_mac[epoch]\n",
    "\n",
    "                    best_fold = fold\n",
    "                    best_epoch = epoch + 1\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                ### Display summary for epoch\n",
    "                print('Epoch {} \\tLearning Rate: {} \\tTime (min): {}'.format(epoch+1, get_lr(optimizer), round((time.time()-start)/60, 2)))\n",
    "                print('Train Loss: {} \\tValidation Loss: {}'. format(round(train_loss[epoch], 4),\n",
    "                                                                     round(val_loss[epoch], 4)))\n",
    "                print('Train Accuracy: {} \\tValidation Accuracy: {}'.format(round(train_acc[epoch], 4),\n",
    "                                                                            round(val_acc[epoch], 4)))\n",
    "                print('Train F1 Mirco: {} \\tValidation F1 Micro: {}'.format(round(train_f1_mic[epoch], 4),\n",
    "                                                                            round(val_f1_mic[epoch], 4)))\n",
    "                print('Train F1 Marco: {} \\tValidation F1 Macro: {}'.format(round(train_f1_mac[epoch], 4),\n",
    "                                                                            round(val_f1_mac[epoch], 4)))\n",
    "                print('Train Precision Mirco: {} \\tValidation Precision Micro: {}'.format(round(train_prec_mic[epoch], 4),\n",
    "                                                                                          round(val_prec_mic[epoch], 4)))\n",
    "                print('Train Precision Marco: {} \\tValidation Precision Macro: {}'.format(round(train_prec_mac[epoch], 4),\n",
    "                                                                                          round(val_prec_mac[epoch], 4)))\n",
    "                print('Train Recall Mirco: {} \\tValidation Recall Micro: {}'.format(round(train_rec_mic[epoch], 4),\n",
    "                                                                                    round(val_rec_mic[epoch], 4)))\n",
    "                print('Train Recall Marco: {} \\tValidation Recall Macro: {}\\n'.format(round(train_rec_mac[epoch], 4),\n",
    "                                                                                    round(val_rec_mac[epoch], 4)))\n",
    "                \n",
    "                \n",
    "                ### Update learning rate if needed\n",
    "                scheduler.step(val_loss[epoch])\n",
    "                \n",
    "                \n",
    "                \n",
    "            ### Display summary graph of fold\n",
    "            fig, (ax1, ax3) = plt.subplots(1,2, figsize = (20,6))\n",
    "            ax1.set_xlabel('Epoch')\n",
    "            ax1.set_ylabel('Loss')\n",
    "            ln1 = ax1.plot(np.arange(start = 1, stop = n_epochs + 1), train_loss, label = 'Train Loss')\n",
    "            ln2 = ax1.plot(np.arange(start = 1, stop = n_epochs + 1), val_loss, label = 'Val Loss')\n",
    "            \n",
    "            ax2 = ax1.twinx()\n",
    "            ax2.set_ylabel('Accuracy')\n",
    "            ln3 = ax2.plot(np.arange(start = 1, stop = n_epochs + 1), train_acc, marker = 'o', label = 'Train Acc')\n",
    "            ln4 = ax2.plot(np.arange(start = 1, stop = n_epochs + 1), val_acc, marker = 'o', label = 'Val Acc')\n",
    "            \n",
    "            lns1 = ln1 + ln2 + ln3 + ln4\n",
    "            labs1 = [l.get_label() for l in lns1]\n",
    "            \n",
    "            ax3.set_xlabel('Epoch')\n",
    "            ax3.set_ylabel('Score')\n",
    "            ln5 = ax3.plot(np.arange(start = 1, stop = n_epochs + 1), train_f1_mic, marker = 'v', label = 'Train F1 Micro')\n",
    "            ln6 = ax3.plot(np.arange(start = 1, stop = n_epochs + 1), val_f1_mic, marker = 'v', label = 'Val F1 Micro')\n",
    "            ln7 = ax3.plot(np.arange(start = 1, stop = n_epochs + 1), train_f1_mac, marker = '^', label = 'Train F1 Macro')\n",
    "            ln8 = ax3.plot(np.arange(start = 1, stop = n_epochs + 1), val_f1_mac, marker = '^', label = 'Val F1 Micro')\n",
    "            ln9 = ax3.plot(np.arange(start = 1, stop = n_epochs + 1), train_prec_mic, marker = 'd', label = 'Train Prec. Micro')\n",
    "            ln10 = ax3.plot(np.arange(start = 1, stop = n_epochs + 1), val_prec_mic, marker = 'd', label = 'Val Prec. Micro')\n",
    "            ln11 = ax3.plot(np.arange(start = 1, stop = n_epochs + 1), train_prec_mac, marker = 'X', label = 'Train Prec. Macro')\n",
    "            ln12 = ax3.plot(np.arange(start = 1, stop = n_epochs + 1), val_prec_mac, marker = 'X', label = 'Val Prec. Macro')\n",
    "            ln13 = ax3.plot(np.arange(start = 1, stop = n_epochs + 1), train_rec_mic, marker = 'P', label = 'Train Rec. Micro')\n",
    "            ln14 = ax3.plot(np.arange(start = 1, stop = n_epochs + 1), val_rec_mic, marker = 'P', label = 'Val Rec. Micro')\n",
    "            ln15 = ax3.plot(np.arange(start = 1, stop = n_epochs + 1), train_rec_mac, marker = 's', label = 'Train Rec. Macro')\n",
    "            ln16 = ax3.plot(np.arange(start = 1, stop = n_epochs + 1), val_rec_mac, marker = 's', label = 'Val Rec. Macro')\n",
    "            \n",
    "            lns2 = ln5 + ln6 + ln7 + ln8 +  ln9 + ln10 + ln11 + ln12 + ln13 + ln14 + ln15 + ln16\n",
    "            labs2 = [l.get_label() for l in lns2]\n",
    "            \n",
    "            \n",
    "            ax1.legend(lns1, labs1, loc = 'upper left', bbox_to_anchor = (1.1,1))\n",
    "            ax3.legend(lns2, labs2, loc = 'upper left', bbox_to_anchor = (1.05,1))\n",
    "            fig.tight_layout()\n",
    "            \n",
    "            plt.show()\n",
    "        \n",
    "        \n",
    "        \n",
    "        ### Display metrics of the best model\n",
    "                \n",
    "        print('------------------------------------------------------------')\n",
    "        print('------------------------------------------------------------')\n",
    "        \n",
    "        print('\\nMetrics of Best Model:')\n",
    "        print('Fold: {} \\tEpoch: {}'.format(best_fold, best_epoch))\n",
    "        print('Train Loss: {} \\tValidation Loss: {}'. format(round(best_train_loss, 4),\n",
    "                                                             round(val_loss_min, 4)))\n",
    "        print('Train Accuracy: {} \\tValidation Accuracy: {}'.format(round(best_train_acc, 4),\n",
    "                                                                    round(best_val_acc, 4)))\n",
    "        print('Train F1 Mirco: {} \\tValidation F1 Micro: {}'.format(round(best_train_f1_mic, 4),\n",
    "                                                                    round(best_val_f1_mic, 4)))\n",
    "        print('Train F1 Marco: {} \\tValidation F1 Macro: {}'.format(round(best_train_f1_mac, 4),\n",
    "                                                                    round(best_val_f1_mac, 4)))\n",
    "        print('Train Precision Mirco: {} \\tValidation Precision Micro: {}'.format(round(best_train_prec_mic, 4),\n",
    "                                                                                  round(best_val_prec_mic, 4)))\n",
    "        print('Train Precision Marco: {} \\tValidation Precision Macro: {}'.format(round(best_train_prec_mac, 4),\n",
    "                                                                                  round(best_val_prec_mac, 4)))\n",
    "        print('Train Recall Mirco: {} \\tValidation Recall Micro: {}'.format(round(best_train_rec_mic, 4),\n",
    "                                                                            round(best_val_rec_mic, 4)))\n",
    "        print('Train Recall Marco: {} \\tValidation Recall Macro: {}'.format(round(best_train_rec_mac, 4),\n",
    "                                                                            round(best_val_rec_mac, 4)))\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet-18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import model\n",
    "resnet18 = models.resnet18(pretrained = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(resnet18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Freeze parameters\n",
    "for param in resnet18.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Change last layer to match output\n",
    "resnet18.fc = nn.Sequential(\n",
    "    nn.Linear(512, 22),\n",
    "    nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(resnet18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import model\n",
    "resnet50 = models.resnet50(pretrained = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(resnet50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Freeze parameters\n",
    "for param in resnet50.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Change last layer to match output\n",
    "resnet50.fc = nn.Sequential(\n",
    "    nn.Linear(2048, 22),\n",
    "    nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(resnet50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet-101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import model\n",
    "resnet101 = models.resnet101(pretrained = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(resnet101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Freeze parameters\n",
    "for param in resnet101.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Change last layer to match output\n",
    "resnet101.fc = nn.Sequential(\n",
    "    nn.Linear(2048, 22),\n",
    "    nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(resnet101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DenseNet-121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import model\n",
    "densenet121 = models.densenet121(pretrained = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(densenet121)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Freeze parameters\n",
    "for param in densenet121.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Change last layer to match output\n",
    "densenet121.classifier = nn.Sequential(\n",
    "    nn.Linear(1024, 22),\n",
    "    nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(densenet121)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DenseNet-169"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import model\n",
    "densenet169 = models.densenet169(pretrained = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(densenet169)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Freeze parameters\n",
    "for param in densenet169.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Change last layer to match output\n",
    "densenet169.classifier = nn.Sequential(\n",
    "    nn.Linear(1664, 22),\n",
    "    nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(densenet169)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DenseNet-201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import model\n",
    "densenet201 = models.densenet201(pretrained = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(densenet201)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Freeze parameters\n",
    "for param in densenet201.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Change last layer to match output\n",
    "densenet201.classifier = nn.Sequential(\n",
    "    nn.Linear(1920, 22),\n",
    "    nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(densenet201)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG11_BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import model\n",
    "vgg11bn = models.vgg11_bn(pretrained = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(vgg11bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Freeze parameters\n",
    "for param in vgg11bn.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Change last layer to match output\n",
    "vgg11bn.classifier[6] = nn.Sequential(\n",
    "    nn.Linear(4096, 22),\n",
    "    nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(vgg11bn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG16_BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import model\n",
    "vgg16bn = models.vgg16_bn(pretrained = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(vgg16bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Freeze parameters\n",
    "for param in vgg16bn.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Change last layer to match output\n",
    "vgg16bn.classifier[6] = nn.Sequential(\n",
    "    nn.Linear(4096, 22),\n",
    "    nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(vgg16bn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG19_BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import model\n",
    "vgg19bn = models.vgg19_bn(pretrained = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(vgg19bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Freeze parameters\n",
    "for param in vgg19bn.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Change last layer to match output\n",
    "vgg19bn.classifier[6] = nn.Sequential(\n",
    "    nn.Linear(4096, 22),\n",
    "    nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(vgg19bn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "# \n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kfold_train_CNN(model = resnet18, training_data = training_data, learning_rate = 0.01, k_folds = 10, n_epochs = 30, model_name = 'resnet18')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kfold_train_CNN(model = resnet50, training_data = training_data, learning_rate = 0.01, k_folds = 10, n_epochs = 30, model_name = 'resnet50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kfold_train_CNN(model = resnet101, training_data = training_data, learning_rate = 0.01, k_folds = 10, n_epochs = 30, model_name = 'resnet101')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kfold_train_CNN(model = densenet121, training_data = training_data, learning_rate = 0.01, k_folds = 10, n_epochs = 30, model_name = 'densenet121')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kfold_train_CNN(model = densenet169, training_data = training_data, learning_rate = 0.01, k_folds = 10, n_epochs = 30, model_name = 'densenet169')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kfold_train_CNN(model = densenet201, training_data = training_data, learning_rate = 0.01, k_folds = 10, n_epochs = 30, model_name = 'densenet201')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kfold_train_CNN(model = vgg11bn, training_data = training_data, learning_rate = 0.01, k_folds = 10, n_epochs = 30, model_name = 'vgg11bn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kfold_train_CNN(model = vgg16bn, training_data = training_data, learning_rate = 0.01, k_folds = 10, n_epochs = 30, model_name = 'vgg16bn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kfold_train_CNN(model = vgg19bn, training_data = training_data, learning_rate = 0.01, k_folds = 10, n_epochs = 30, model_name = 'vgg19bn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(time.time() - START) / 60"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
