{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json \n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "import re\n",
    "import emoji\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from hatesonar import Sonar # This is the hate speech detection library; it is based on bert\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json_object(filename):\n",
    "    with open(file, \"r\", encoding='utf-8') as read_file:\n",
    "        json_array = json.load(read_file)\n",
    "    json_data = []    \n",
    "    #parse the data\n",
    "    for item in json_array:\n",
    "        details = {\"id\":None, \"labels\":None, \"text\":None}\n",
    "        details['id'] = item['id']\n",
    "        try: \n",
    "            details['labels'] = item['labels']\n",
    "        except KeyError: \n",
    "            details['labels'] = []         \n",
    "        details['text'] = item['text']\n",
    "        json_data.append(details)\n",
    "\n",
    "    \n",
    "    return json_data\n",
    "\n",
    "\n",
    "def text_preprocessing(text):\n",
    "    \"\"\"\n",
    "    - Remove entity mentions (eg. '@united')\n",
    "    - Correct errors (eg. '&amp;' to '&')\n",
    "    @param    text (str): a string to be processed.\n",
    "    @return   text (Str): the processed string.\n",
    "    \"\"\"\n",
    "    # Remove '@name'\n",
    "   # text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
    "\n",
    "    # Replace '&amp;' with '&'\n",
    "    text = re.sub(r'&amp;', '&', text)\n",
    "\n",
    "    # Remove trailing whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    #remove html\n",
    "   # text = re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\", \"\", text)\n",
    "    \n",
    "    #remove emoji\n",
    "   # text = ''.join(c for c in text if c not in emoji.UNICODE_EMOJI) #Remove Emojis\n",
    "    \n",
    "    # remove numbers\n",
    "    # text = re.sub('[0-9]+', '', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def hate_speech_classifier(df, Class, hate, offensive, neither):\n",
    "    for i in df['text']:\n",
    "        sonar_dict = sonar.ping(text=i)\n",
    "        Class.append(list(sonar_dict.values())[1])\n",
    "        hate.append(list(list(sonar_dict.values())[2][0].values())[1])\n",
    "        offensive.append(list(list(sonar_dict.values())[2][1].values())[1])\n",
    "        neither.append(list(list(sonar_dict.values())[2][2].values())[1])\n",
    "\n",
    "\n",
    "def sentiment_classifier(df):\n",
    "    for i,text in enumerate(df['text']):\n",
    "        result = sentimentanalyzer(text)[0]\n",
    "        df.loc[[i],'sentiment_class'] = result['label']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file paths\n",
    "lst_file_path=[]\n",
    "\n",
    "lst_file_path.append(\"../data/training_data_task3.txt\")\n",
    "\n",
    "lst_file_path.append(\"../data/validation_data_task3.txt\")\n",
    "\n",
    "\n",
    "# List of keys \n",
    "dataset = [\"training\", \"validation\"] \n",
    "# empty dictionary\n",
    "dic_datasets =  dict.fromkeys(dataset, pd.DataFrame()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading../data/training_data_task3.txt\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/training_data_task3.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-8211162f101f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlst_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loading'\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdic_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_json_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_json_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-58c5beaee94f>\u001b[0m in \u001b[0;36mextract_json_object\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextract_json_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mread_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mjson_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mjson_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#parse the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/training_data_task3.txt'"
     ]
    }
   ],
   "source": [
    "for i, file in enumerate(lst_file_path):\n",
    "    print('loading'+ file)\n",
    "    dic_datasets[dataset[i]] = pd.DataFrame(extract_json_object(extract_json_object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training = pd.DataFrame(dic_datasets['training'][['id','labels','text']])\n",
    "df_training.labels = df_training.labels.apply(lambda y: ['no_class'] if len(y)==0 else y)\n",
    "\n",
    "df_val = pd.DataFrame(dic_datasets['validation'][['id','labels','text']])\n",
    "df_val.labels = df_val.labels.apply(lambda y: ['no_class'] if len(y)==0 else y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create auxilary features (hate speech indicator and sentiment )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an object of Sonar Hate Speech Detection\n",
    "sonar = Sonar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_speech_class = []\n",
    "hate = []\n",
    "offensive = []\n",
    "neither = []\n",
    "\n",
    "#Function calling \n",
    "hate_speech_classifier(df_training, hate_speech_class, hate, offensive, neither)\n",
    "\n",
    "# Prepare columns to add the scores later\n",
    "df_training[\"hate_speech_class\"] = hate_speech_class\n",
    "#df_training[\"hate\"] = hate\n",
    "#df_training[\"offensive\"] = offensive\n",
    "#df_training[\"neither\"] = neither\n",
    "df_training.hate_speech_class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_speech_class = []\n",
    "hate = []\n",
    "offensive = []\n",
    "neither = []\n",
    "\n",
    "hate_speech_classifier(df_val, hate_speech_class, hate, offensive, neither)\n",
    "\n",
    "# Prepare columns to add the scores later\n",
    "df_val[\"hate_speech_class\"] = hate_speech_class\n",
    "\n",
    "#df_val[\"hate\"] = hate\n",
    "#df_val[\"offensive\"] = offensive\n",
    "#df_val[\"neither\"] = neither\n",
    "\n",
    "df_val.hate_speech_class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment analysis using distillbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentimentanalyzer = pipeline(\"sentiment-analysis\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_speech_class = []\n",
    "hate = []\n",
    "offensive = []\n",
    "neither = []\n",
    "\n",
    "#Function calling \n",
    "hate_speech_classifier(df_training, hate_speech_class, hate, offensive, neither)\n",
    "\n",
    "# Prepare columns to add the scores later\n",
    "df_training[\"hate_speech_class\"] = hate_speech_class\n",
    "#df_training[\"hate\"] = hate\n",
    "#df_training[\"offensive\"] = offensive\n",
    "#df_training[\"neither\"] = neither\n",
    "df_training.hate_speech_class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_speech_class = []\n",
    "hate = []\n",
    "offensive = []\n",
    "neither = []\n",
    "\n",
    "hate_speech_classifier(df_val, hate_speech_class, hate, offensive, neither)\n",
    "\n",
    "# Prepare columns to add the scores later\n",
    "df_val[\"hate_speech_class\"] = hate_speech_class\n",
    "\n",
    "#df_val[\"hate\"] = hate\n",
    "#df_val[\"offensive\"] = offensive\n",
    "#df_val[\"neither\"] = neither\n",
    "\n",
    "df_val.hate_speech_class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment analysis using distillbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentimentanalyzer = pipeline(\"sentiment-analysis\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate sentiment\n",
    "df_training = sentiment_classifier(df_training)\n",
    "df_val = sentiment_classifier(df_val)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hot encode auxilary variables\n",
    "df_training = pd.get_dummies(df_training, columns =[\"hate_speech_class\",\"sentiment_class\"])\n",
    "df_val = pd.get_dummies(df_val, columns =[\"hate_speech_class\",\"sentiment_class\"])\n",
    "\n",
    "# hot encode multi - labels (dependant variable)\n",
    "mlb = MultiLabelBinarizer(sparse_output=True)\n",
    "\n",
    "df_concat_labels = pd.concat([df_training.pop('labels'), df_val.pop('labels')])\n",
    "concat_labels = df_training.join(pd.DataFrame.sparse.from_spmatrix(\n",
    "                    mlb.fit_transform(df_concat_labels),\n",
    "                    index=df_concat_labels.index,\n",
    "                    columns=mlb.classes_))\n",
    "\n",
    "\n",
    "df_training = concat_labels[0:len(df_training)]\n",
    "df_training.reset_index(inplace=True)\n",
    "\n",
    "df_val = concat_labels[len(df_training):]\n",
    "df_val.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.comment_text = dataframe.text\n",
    "        \n",
    "        self.hate = dataframe[['hate_speech_class_hate_speech','hate_speech_class_neither', 'hate_speech_class_offensive_language']].to_numpy()\n",
    "        self.sentiment = dataframe[['sentiment_class_NEGATIVE','sentiment_class_POSITIVE']].to_numpy()\n",
    "        \n",
    "        self.targets = self.data[[\n",
    "                                    'Appeal to authority',\n",
    "                                    'Appeal to fear/prejudice',\n",
    "                                    'Black-and-white Fallacy/Dictatorship',\n",
    "                                    'Causal Oversimplification',\n",
    "                                    'Doubt',\n",
    "                                    'Exaggeration/Minimisation',\n",
    "                                    'Flag-waving',\n",
    "                                    'Glittering generalities (Virtue)',\n",
    "                                    'Loaded Language',\n",
    "                                    'Misrepresentation of Someone\\'s Position (Straw Man)',\n",
    "                                    'Name calling/Labeling',\n",
    "                                    'Obfuscation, Intentional vagueness, Confusion',\n",
    "                                    'Presenting Irrelevant Data (Red Herring)',\n",
    "                                    'Reductio ad hitlerum',\n",
    "                                    'Repetition',\n",
    "                                    'Slogans',\n",
    "                                    'Smears',\n",
    "                                    'Thought-terminating cliché',\n",
    "                                    'Whataboutism',\n",
    "                                    'Bandwagon',\n",
    "                                    'Transfer',\n",
    "                                    'Appeal to (Strong) Emotions'\n",
    "                                ]].to_numpy()\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.comment_text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #print(index)\n",
    "        #print(self.comment_text.index)\n",
    "        comment_text = str(self.comment_text[index])\n",
    "        comment_text = \" \".join(comment_text.split())\n",
    "\n",
    "        # inputs = self.tokenizer.encode_plus(\n",
    "        #    comment_text,\n",
    "        #    None,\n",
    "        #    add_special_tokens=True,\n",
    "        #    truncation=True,\n",
    "        #    max_length=self.max_len,\n",
    "        #    pad_to_max_length=True,\n",
    "        #    #padding=True,\n",
    "        #    #padding='longest',\n",
    "        #    return_token_type_ids=True\n",
    "        # )\n",
    "        #inputs = tokenizer.encode_plus(\n",
    "        #            comment_text, \n",
    "        #            add_special_tokens = True,    \n",
    "        #            truncation = False, \n",
    "        #            max_length=self.max_len,\n",
    "        #            padding = \"max_length\",\n",
    "                    #padding_side='right',\n",
    "                   # return_attention_mask = True, \n",
    "        #            return_tensors = \"pt\",\n",
    "        #            return_token_type_ids=True,    \n",
    "        #)\n",
    "        \n",
    "        inputs = tokenizer.encode_plus(\n",
    "                    comment_text, \n",
    "                    text_pair = None,\n",
    "                    add_special_tokens = True,    \n",
    "                    max_length=self.max_len,\n",
    "                    padding = \"max_length\",\n",
    "                    pad_to_max_length = True,\n",
    "                    return_token_type_ids=True,\n",
    "                    truncation=True\n",
    "        )\n",
    "        \n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.float),\n",
    "            'hate': torch.tensor(self.hate[index], dtype=torch.long), \n",
    "            'sentiment': torch.tensor(self.sentiment[index], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper functions\n",
    "\n",
    "def create_data_loader_kfold(df_kf, df_val, trn_ids, tst_ids):\n",
    "    print('Original Train Dataset: ' + str(len(df_kf)))\n",
    "    \n",
    "    cust_Dataset_train = CustomDataset(df_kf, tokenizer, MAX_LEN)\n",
    "    cust_Dataset_val = CustomDataset(df_val, tokenizer, MAX_LEN)\n",
    "    \n",
    "\n",
    "    # Sample elements randomly from a given list of ids, no replacement.\n",
    "    kfold_train_subsampler = torch.utils.data.SubsetRandomSampler(trn_ids)\n",
    "    kfold_test_subsampler = torch.utils.data.SubsetRandomSampler(tst_ids)\n",
    "    \n",
    "    val_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                    'shuffle': True,\n",
    "                    'num_workers': 0\n",
    "                    }\n",
    "    \n",
    "    \n",
    "    # Define data loaders for training and testing data in this fold\n",
    "    kfold_trainloader = torch.utils.data.DataLoader(\n",
    "                      cust_Dataset_train, \n",
    "                      batch_size=VALID_BATCH_SIZE, sampler=kfold_train_subsampler)\n",
    "    kfold_testloader = torch.utils.data.DataLoader(\n",
    "                      cust_Dataset_train,\n",
    "                      batch_size=VALID_BATCH_SIZE, sampler=kfold_test_subsampler)\n",
    "    \n",
    "    valloader = DataLoader(cust_Dataset_val, **val_params)\n",
    "    \n",
    "\n",
    "    print(\"KFOLD CROSSVALIDATION TRAIN Dataset: {}\".format(len(kfold_trainloader) * kfold_trainloader.batch_size))\n",
    "    print(\"KFOLD CROSSVALIDATION TEST Dataset: {}\".format(len(kfold_testloader) * kfold_trainloader.batch_size))\n",
    "    print(\"VALIDATION Dataset: {}\".format(len(valloader) * valloader.batch_size))\n",
    "    \n",
    "    \n",
    "    return kfold_trainloader, kfold_testloader, valloader\n",
    "     \n",
    "\n",
    "def create_data_loader(train_dataset,test_dataset):\n",
    "    print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "    print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
    "    \n",
    "    training_set = CustomDataset(train_dataset, tokenizer, MAX_LEN)\n",
    "    testing_set = CustomDataset(test_dataset, tokenizer, MAX_LEN)\n",
    "    dataset_size = len(train_dataset)\n",
    "    \n",
    "    train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "    test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                    'shuffle': True,\n",
    "                    'num_workers': 0\n",
    "                    }\n",
    "\n",
    "    training_loader = DataLoader(training_set, **train_params)\n",
    "    testing_loader = DataLoader(testing_set, **test_params)\n",
    "\n",
    "\n",
    "    return training_loader, testing_loader, dataset_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, testing_loader):\n",
    "    # Put model in evaluation mode to evaluate loss on the validation set\n",
    "    model.eval()\n",
    "\n",
    "    #track variables\n",
    "    true_labels,pred_labels = [],[]\n",
    "\n",
    "    # Predict\n",
    "    for _, data in enumerate(testing_loader, 0):\n",
    "        #print(_)\n",
    "        \n",
    "        #prepare data to feed into model\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        \n",
    "        hate = data['hate'].to(device, dtype = torch.long)\n",
    "        sent = data['sentiment'\n",
    "                   ].to(device, dtype = torch.long)\n",
    "                \n",
    "        targets = data['targets'].to(device, dtype = torch.float)\n",
    "        \n",
    "       \n",
    "        with torch.no_grad():\n",
    "           \n",
    "            \n",
    "            # Forward pass\n",
    "            outputs, model_output_object = model(ids, mask, None, hate, sent)\n",
    "            b_logit_pred = outputs\n",
    "            pred_label = b_logit_pred\n",
    "\n",
    "            b_logit_pred = b_logit_pred.detach().cpu().numpy()\n",
    "            pred_label = pred_label.to('cpu').numpy()\n",
    "            targets = targets.to('cpu').numpy()\n",
    "\n",
    "        #tokenized_texts.append(b_input_ids)\n",
    "        #logit_preds.append(b_logit_pred)\n",
    "        true_labels.append(targets)\n",
    "        pred_labels.append(pred_label)\n",
    "\n",
    "    # Flatten outputs\n",
    "    #tokenized_texts = [item for sublist in tokenized_texts for item in sublist]\n",
    "    # print(true_labels)\n",
    "    # print(pred_labels)\n",
    "    pred_labels = [item for sublist in pred_labels for item in sublist]\n",
    "    true_labels = [item for sublist in true_labels for item in sublist]\n",
    "    # Converting flattened binary values to boolean values\n",
    "    true_bools = [tl==1 for tl in targets]\n",
    "    \n",
    "    return true_labels, true_bools, pred_labels, model_output_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
