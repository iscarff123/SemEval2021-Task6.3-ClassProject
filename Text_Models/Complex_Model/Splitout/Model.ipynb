{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import KFold\n",
    "import torch\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler, ConcatDataset\n",
    "import torch.optim as optim\n",
    "from torch.optim import Optimizer\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "\n",
    "\n",
    "# !pip install transformers\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertModel, BertConfig, AdamW\n",
    "from transformers import DistilBertTokenizer, DistilBertModel, DistilBertConfig\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "\n",
    "# ensure reproducability\n",
    "#torch.seed(25)\n",
    "torch.manual_seed(25)\n",
    "torch.cuda.manual_seed_all(25)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(25)\n",
    "random.seed(25)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Setting up the device for GPU usage\n",
    "\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "#device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sections of config\n",
    "\n",
    "# Defining some key variables that will be used later on in the training\n",
    "model_type = 'distilbert'\n",
    "model_version = 'distilbert-base-uncased'\n",
    "do_lower_case = True\n",
    "\n",
    "freeze_layer_count = 11\n",
    "\n",
    "\n",
    "MAX_LEN = 128  #128 works ok\n",
    "\n",
    "TRAIN_BATCH_SIZE = 6 #10 works ok # 32 works \n",
    "VALID_BATCH_SIZE = 6\n",
    "\n",
    "EPOCHS = 7 # 15 works (14)\n",
    "\n",
    "#LEARNING_RATE = 1e-05\n",
    "LEARNING_RATE = None\n",
    "#tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=do_lower_case)\n",
    "#tokenizer = DistilBertTokenizer.from_pretrained(model_version, do_lower_case=do_lower_case, output_hidden_states=True)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_version, do_lower_case=do_lower_case)\n",
    "\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "# kfold Configuration options\n",
    "k_folds = 15 #(10)\n",
    "  \n",
    "# For fold results\n",
    "results = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initializing a Distillbert bert-base-uncased style configuration\n",
    "# SECOND SET OF Hyper Parameters\n",
    "#vocab_size= 30522\n",
    "#n_heads=12\n",
    "# hidden_dim=3072\n",
    "#dropout = .1\n",
    "# attention_dropout = .1\n",
    "config = transformers.DistilBertConfig(vocab_size= 30522, max_position_embeddings=1024, sinusoidal_pos_embds=False, \n",
    "                           n_layers=6, n_heads=6 , dim=768, hidden_dim=3072, dropout=0.1, attention_dropout=0.1, activation='gelu', \n",
    "                           initializer_range=0.02, qa_dropout=0.1, seq_classif_dropout=0.2, pad_token_id= 0, use_cache=True,  output_hidden_states=True)\n",
    "\n",
    "#DistilBertConfig.from_pretrained('distilbert-base-cased', output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistilBertClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DistilBertClass, self).__init__()\n",
    "            \n",
    "        self.concat = 0\n",
    "        #self.l1 = transformers.BertModel.from_pretrained('bert-base-uncased', output_attentions=True)\n",
    "        #self.l1 = transformers.BertModel.from_pretrained('bert-base-uncased', output_attentions=False)\n",
    "        \n",
    "        self.l1 = transformers.DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        #lock down for transfer learned laayers\n",
    "        for param in  self.l1.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # now add new layers\n",
    "        self.concat = 3 # 1 used base data # 2 base + derived hate  #3 uses base + derived hate + derived sentiment  #4 base + derived  sentiment  # else default to base\n",
    "        \n",
    "        if self.concat == 1: \n",
    "            dim = 768\n",
    "        elif self.concat == 2:\n",
    "            dim = 771\n",
    "        elif self.concat == 3:\n",
    "            dim = 773\n",
    "        elif self.concat == 4:\n",
    "            dim = 770\n",
    "        else: \n",
    "            dim = 768\n",
    "            \n",
    "        \n",
    "        self.pre_classifier = torch.nn.Linear(dim,1024) #768\n",
    "        \n",
    "      #  self.fc2 = torch.nn.Linear(756, 128)    \n",
    "       # self.fc3 = torch.nn.Linear(1024, 512)\n",
    "       # self.fc4 = torch.nn.Linear(512, 128)\n",
    "        \n",
    "        self.classifier = torch.nn.Linear(1024, 22)\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(0.2) #.3\n",
    "        \n",
    "    #768\n",
    "    # def forward(self, ids, mask, token_type_ids):\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids,  hate,sent):\n",
    "        # model inputs\n",
    "        # (input_ids=None, attention_mask=None, head_mask=None, inputs_embeds=None, output_attentions=None, output_hidden_states=None, return_dict=None)\n",
    "        \n",
    "        #print(ids)\n",
    "        #print(mask)\n",
    "        #_,\n",
    "        #print(type(self.l1(input_ids =ids, attention_mask = mask, token_type_ids = token_type_ids)))\n",
    "        #print(self.l1(input_ids =ids, attention_mask = mask, token_type_ids = token_type_ids))\n",
    "        \n",
    "        # transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions -> out of self.l1, need the tensor poooler_output\n",
    "        #output_1= self.l1(input_ids =ids, attention_mask = mask, token_type_ids = token_type_ids).pooler_output\n",
    "        #output_1 = self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids)\n",
    "        #print(self.l1)\n",
    "        #print(type(self.l1))\n",
    "        #print(self.l1)\n",
    "        \n",
    "        #output_1 = self.l1(ids, attention_mask = mask)\n",
    "        #output_1 = self.l1(input_ids = ids, attention_mask = mask)\n",
    "\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask =attention_mask)\n",
    "        #self.l1()\n",
    "        #print(type(output_1))\n",
    "        #print(output_1)\n",
    "        #print(_)\n",
    "        #print(type(_))\n",
    "    \n",
    "        # ONLY GET LAST HIDDEN STATE\n",
    "\n",
    "        #output_2 = self.l2(output_1.last_hidden_state)\n",
    "        #output_2 = self.l2(output_1)\n",
    "        #output = self.l3(output_2)\n",
    "        #print(output.shape)\n",
    "        #output = output[:,0,:].numpy()\n",
    "        \n",
    "        hidden_state = output_1[0]\n",
    "        x1 = hidden_state[:, 0]\n",
    "        \n",
    "        # prepare hidden state to append additional features (hate and sentiment; hotencoded)\n",
    "        x1 = x1.view(x1.size(0), -1)\n",
    "        #print(x1.shape)\n",
    "        x2 = hate\n",
    "        #print(x2.shape)\n",
    "        x3 = sent\n",
    "        \n",
    "        if self.concat == 1:\n",
    "            x = x1\n",
    "            #print(x.shape)\n",
    "        elif self.concat == 2: \n",
    "            x = torch.cat((x1, x2), dim = 1)\n",
    "        elif self.concat == 3:\n",
    "            x = torch.cat((x1, x2, x3), dim=1)\n",
    "        elif self.concat == 4:  \n",
    "            x = torch.cat((x1, x3), dim=1)\n",
    "        else:\n",
    "            x = hidden_state[:, 0]\n",
    "            \n",
    "        \n",
    "        #print(x1.shape)\n",
    "        #print(x2.shape)\n",
    "        #print(x3.shape)\n",
    "        #print(x.shape)\n",
    "        \n",
    "        #print(x2)\n",
    "        #print(x3)\n",
    "        \n",
    "        fc_output = x\n",
    "        #print(fc_output.shape)\n",
    "        # fc_output = hidden_state[:, 0]\n",
    "        fc_output = self.pre_classifier(fc_output)\n",
    "        \n",
    "       # fc_output = torch.nn.GELU()(fc_output)\n",
    "       # fc_output = self.dropout(fc_output)\n",
    "\n",
    "        #pooler = torch.nn.Tanh()(pooler)\n",
    "        #pooler = torch.nn.ReLU()(pooler)\n",
    "        #pooler = self.dropout(pooler)\n",
    "        \n",
    "        \n",
    "        #pooler = torch.nn.ReLU()(pooler)\n",
    "        #pooler = self.dropout(pooler)\n",
    "        \n",
    "        #output = self.classifier(pooler)\n",
    "        \n",
    "        \n",
    "        #print(fc_output.shape)\n",
    "    #    fc_output = self.fc2(fc_output)\n",
    "    #    fc_output = torch.nn.GELU()(fc_output)\n",
    "    #    fc_output = self.dropout(fc_output)\n",
    "        \n",
    "        #GELU\n",
    "        \n",
    "     #   fc_output = self.fc3(fc_output)\n",
    "    #    fc_output = torch.nn.GELU()(fc_output)\n",
    "    #    fc_output = self.dropout(fc_output)\n",
    "        \n",
    "        \n",
    "    #    fc_output = self.fc2(fc_output)\n",
    "        fc_output = torch.nn.GELU()(fc_output)  # using gelu except for the last one to allow for classification\n",
    "        fc_output = self.dropout(fc_output)\n",
    "        \n",
    "        output = self.classifier(fc_output)\n",
    "\n",
    "        #output = torch.softmax(output, dim = -1)\n",
    "        #print(output)\n",
    "        #print(output1)\n",
    "        \n",
    "        #pooler = self.dropout(torch.nn.GELU())\n",
    "        #pooler = self.dropout(torch.nn.GELU(self.fc3(pooler)))\n",
    "        #pooler = self.dropout(torch.nn.GELU(self.fc4(pooler)))\n",
    "        #output = self.classifier(pooler)\n",
    "        \n",
    "        return output, output_1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def instantiate_model(config, lock_layer_count):\n",
    "    #model = DistilBertClass(config=config)\n",
    "    model = DistilBertClass()\n",
    "    # Accessing the model configuration\n",
    "    #configuration = model.config\n",
    "    \n",
    "    # Log metrics with wandb\n",
    "    #wandb.watch(model)\n",
    "\n",
    "\n",
    "    #freeze model\n",
    "  #  if lock_layer_count:\n",
    "    # We freeze here the embeddings of the model\n",
    "    #    for param in model.l1.embeddings.parameters():\n",
    "   #         param.requires_grad = False\n",
    "            \n",
    "     #   for param in model.l1.parameters():\n",
    "     #       param.requires_grad = False\n",
    "            \n",
    "\n",
    "\n",
    "       # if lock_layer_count != -1:\n",
    "            # if freeze_layer_count == -1, we only freeze the embedding layer\n",
    "            # otherwise we freeze the first `freeze_layer_count` encoder layers\n",
    "        #    for layer in model.l1.encoder.layer[:lock_layer_count]:\n",
    "         #       for param in layer.parameters():\n",
    "         #           param.requires_grad = False\n",
    "\n",
    "    # view layers                \n",
    "    #for name, param in model.named_parameters():\n",
    "        #print(name)\n",
    "    #    if param.requires_grad:\n",
    "    #        print(name)\n",
    "\n",
    "    # learnable parameters\n",
    "    pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(\"Total Learnable Parms: \" + str(pytorch_total_params))\n",
    "    \n",
    "    return model #, configuration\n",
    "\n",
    "\n",
    "# contains a sigmoids activation function built in.\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html\n",
    "\n",
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Learnable Parms: 815126\n"
     ]
    }
   ],
   "source": [
    "model = instantiate_model(config,freeze_layer_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre_classifier.weight\n",
      "pre_classifier.bias\n",
      "classifier.weight\n",
      "classifier.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    #print(name)\n",
    "    if param.requires_grad:\n",
    "        print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace AdamW with Adafactor\n",
    "\n",
    "optimizer = transformers.Adafactor(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE,  # when using warm up and relative step, LR is auto determined\n",
    "    #lr=1e-3,\n",
    "    eps=(1e-30, 1e-3),\n",
    "    clip_threshold=1.0,\n",
    "    decay_rate=-0.8, #0.8\n",
    "    beta1=None, # <- used for L1 regularization\n",
    "    weight_decay=0.0002, # L2 regularization, to prevent overfitting  (beta2)\n",
    "    relative_step=True,\n",
    "    scale_parameter=True, # https://github.com/pytorch/pytorch/issues/25081 this setting keeps the gradients from reaching 0 (using the clip threshold) (if this is enabled, must modify in training)\n",
    "    warmup_init=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=k_folds, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Combined Dataset: {}\".format(len(df_combined)))\n",
    "\n",
    "print(\"Train Dataset: {}\".format(len(df_training)))\n",
    "print(\"Val Dataset: {}\".format(len(df_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, trn_loader, tst_loader, optimizer, num_epochs=5):\n",
    "    scaler = torch.cuda.amp.GradScaler() # used to minimize model footprint, strategy is called model quantitization.\n",
    "    #model.train()\n",
    "    torch.cuda.empty_cache()\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    losses_train = []\n",
    "    losses_val = []\n",
    "    accuracy_train = []\n",
    "    accuracy_val= []\n",
    "    clip = 1\n",
    "    \n",
    "    #scheduler = transformers.get_linear_schedule_with_warmup(optimizer, num_warmup_steps=3, num_training_steps=3)\n",
    "    num_training_steps = num_epochs+1\n",
    "    num_warmup_steps = 2\n",
    "    \n",
    "    #scheduler = transformers.get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps) #bug is not allowing for this to work :(\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "       \n",
    "       \n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 20)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "                loader = trn_loader\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "                loader = tst_loader\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for _, data in enumerate(loader):\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                #input into model (takes 3 items: ids, mask and token)\n",
    "                ids = data['ids'].to(device, dtype = torch.long)\n",
    "                mask = data['mask'].to(device, dtype = torch.long)\n",
    "                token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "                #print(type(data))\n",
    "                #print(data)\n",
    "                #print(data.keys())\n",
    "                hate = data['hate'].to(device, dtype = torch.long)\n",
    "                sent = data['sentiment'].to(device, dtype = torch.long)\n",
    "                \n",
    "            \n",
    "                # gold label\n",
    "                labels = data['targets'].to(device)\n",
    "\n",
    "                \n",
    "                # forward\n",
    "                # track history if only in train                \n",
    "                with torch.cuda.amp.autocast():  # used to minimize model footprint, strategy is called model quantitization.\n",
    "                    #print('id')\n",
    "                    #print(ids)\n",
    "                    #print('mask')\n",
    "                    #print(mask)\n",
    "                    #print('token')\n",
    "                    #print(token_type_ids)\n",
    "                    \n",
    "                    \n",
    "                    preds, model_output_object  = model(ids, mask, None, hate, sent)\n",
    "                    #loss, preds, hidden_states_output, attention_mask_output = model(ids, mask, token_type_ids)\n",
    "                    #print(type(preds))\n",
    "                    #print(preds.info())\n",
    "                    features = preds\n",
    "                    #features = preds[:,0,:].cpu()\n",
    "                    #features = features.to('cuda')\n",
    "                    #time.sleep(5)\n",
    "                    # model output\n",
    "                    # 'ids': torch.tensor(ids, dtype=torch.long),\n",
    "                    # 'mask': torch.tensor(mask, dtype=torch.long),\n",
    "                    # 'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "                    #'targets'\n",
    "                    #print(preds[1].shape)\n",
    "                    \n",
    "                    #print('pred')\n",
    "                    #print(preds)\n",
    "                    loss = loss_fn(preds, labels) \n",
    "                    #print('loss')\n",
    "                    #print(loss)\n",
    "                    #preds = preds.detach().cpu().numpy()\n",
    "                \n",
    "                    #print(loss)\n",
    "                    \n",
    "                    # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    #scaler.scale(loss).backward() # used to minimize model footprint, strategy is called model quantitization.\n",
    "                    #print(loss)\n",
    "                    #scaler.step(optimizer)\n",
    "                    #unsale and clip to remove any possible inf or nulls from result set; this can happen due to scaling using 8 or 16 bit vs 32 or 64.\n",
    "                    \n",
    "                    #added this try except block to catch gradient that become inf or NaN. (supposedly this is automatic but google says there is currently a bug)\n",
    "                    #implemented gradient scaling in the try block\n",
    "                    \n",
    "                    try:\n",
    "                        optimizer.zero_grad()\n",
    "                        # Scales loss.  Calls backward() on scaled loss to create scaled gradients.\n",
    "                        # Backward passes under autocast are not recommended.\n",
    "                        #Backward ops run in the same dtype autocast chose for corresponding forward ops.\n",
    "                       # scaler.scale(loss).backward()\n",
    "                        \n",
    "                        # scaler.step() first unscales the gradients of the optimizer's assigned params.\n",
    "                        # If these gradients do not contain infs or NaNs, optimizer.step() is then called,\n",
    "                        # otherwise, optimizer.step() is skipped.\n",
    "                       # scaler.step(optimizer)\n",
    "                        \n",
    "                        # Updates the scale for next iteration.\n",
    "                       # scaler.update()\n",
    "                        \n",
    "                        \n",
    "                        scaler.scale(loss).backward()\n",
    "\n",
    "                        # Unscales the gradients of optimizer's assigned params in-place\n",
    "                        scaler.unscale_(optimizer)\n",
    "\n",
    "                        # Since the gradients of optimizer's assigned params are unscaled, clips as usual:\n",
    "                        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.25)\n",
    "                        # optimizer's gradients are already unscaled, so scaler.step does not unscale them,\n",
    "                        # although it still skips optimizer.step() if the gradients contain infs or NaNs.\n",
    "                        scaler.step(optimizer)\n",
    "\n",
    "                        # Updates the scale for next iteration.\n",
    "                        scaler.update()\n",
    "        \n",
    "                        # scheduler.step()  # Update learning rate schedule  # bug is not letting this part work either.\n",
    "                        optimizer.step()\n",
    "                        #optimizer.zero_grad()\n",
    "                        \n",
    "                    except AssertionError:\n",
    "                            print(\"scaler encountered inf, did not step, will try next iteration.\")\n",
    "                            pass\n",
    "                    \n",
    "                    \n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.detach().cpu().numpy() * ids.size(0)\n",
    "                #print(running_loss)\n",
    "                running_corrects += torch.sum(features.data == labels.data).detach().cpu().numpy()\n",
    "                \n",
    "                \n",
    "\n",
    "                #print(running_corrects)\n",
    "               # print(preds.detach().cpu().numpy())\n",
    "                #print(labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / (len(loader) * loader.batch_size)\n",
    "            # epoch_acc = running_corrects/ (len(loader) * loader.batch_size)\n",
    "            \n",
    "            ac_labels = labels.detach().cpu().numpy()\n",
    "            ac_pred =outputs = [pl>0.50 for pl in features.detach().cpu().numpy()]\n",
    "            epoch_acc =  accuracy = metrics.accuracy_score(ac_labels, ac_pred) \n",
    "           \n",
    "\n",
    "            \n",
    "            if phase == 'train':\n",
    "                losses_train.append(epoch_loss) \n",
    "                accuracy_train.append(epoch_acc)\n",
    "            else:\n",
    "                losses_val.append(epoch_loss)\n",
    "                accuracy_val.append(epoch_acc)\n",
    "                wandb.log({\"Test Accuracy\": accuracy_val, \"Test Loss\": losses_val})\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "           \n",
    "     \n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "        \n",
    "        time_elapsed = time.time() - since\n",
    "        print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "        print('Best val Acc: {:4f}'.format(best_acc))\n",
    "        internal_lr = 0.00\n",
    "        if(optimizer.param_groups[0][\"lr\"] == None):\n",
    "            internal_lr = 0.00\n",
    "        else:\n",
    "            internal_lr = optimizer.param_groups[0][\"lr\"] \n",
    "        print('Adjusted - Learning Rate:  {:.5f}'.format(internal_lr))\n",
    "        # load best model weights\n",
    "        model.load_state_dict(best_model_wts)\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': epoch_loss,\n",
    "            }, \"./model.pkl\")\n",
    "    return model, losses_train, losses_val, accuracy_train, accuracy_val, model_output_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "for fold, (kfold_cv_train_ids, kfold_cv_test_ids) in enumerate(kfold.split(df_training)):\n",
    "\n",
    "    # Print\n",
    "    print(f'FOLD {fold}')\n",
    "    print('--------------------------------')\n",
    "    \n",
    "    # Sample elements randomly from a given list of ids, no replacement.\n",
    "    #train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    #test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "    \n",
    "    # Define data loaders for training and testing data in this fold\n",
    "    #trainloader = torch.utils.data.DataLoader(\n",
    "    #                  dataset, \n",
    "    #                  batch_size=10, sampler=train_subsampler)\n",
    "    #testloader = torch.utils.data.DataLoader(\n",
    "    #                  dataset,\n",
    "    #                  batch_size=10, sampler=test_subsampler)\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    kfold_cv_train_loader, kfold_cv_test_loader, val_loader = create_data_loader_kfold(df_training, df_val, kfold_cv_train_ids, kfold_cv_test_ids)\n",
    "    # Init the neural network\n",
    "    #network = SimpleConvNet()\n",
    "    \n",
    "    # Initialize optimizer\n",
    "    #optimizer = torch.optim.Adam(network.parameters(), lr=1e-4)\n",
    "\n",
    "    \n",
    "    \n",
    "    model, loss_train, loss_val, accuracy_train,accuracy_val, model_output_object_train = train_model(model, kfold_cv_train_loader, kfold_cv_test_loader, optimizer, num_epochs=EPOCHS)\n",
    "  \n",
    "            \n",
    "    # Process is complete.\n",
    "    print('Training process has finished. Saving trained model.')\n",
    "\n",
    "    # Print about testing\n",
    "    print('Starting Validation')\n",
    "    \n",
    "    # Saving the model\n",
    "    save_path = f'./model-fold-{fold}.pth'\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    torch.save(model.state_dict(), os.path.join(wandb.run.dir, save_path))\n",
    "\n",
    "    # Evaluationfor this fold\n",
    "    correct, total = 0, 0\n",
    "    #with torch.no_grad():\n",
    "\n",
    "      # Iterate over the test data and generate predictions\n",
    "      #for i, data in enumerate(testloader, 0):\n",
    "\n",
    "        # Get inputs\n",
    "       # inputs, targets = data\n",
    "\n",
    "        # Generate outputs\n",
    "        #outputs = network(inputs)\n",
    "\n",
    "        # Set total and correct\n",
    "       # _, predicted = torch.max(outputs.data, 1)\n",
    "       # total += targets.size(0)\n",
    "       # correct += (predicted == targets).sum().item()\n",
    "    \n",
    "    true_labels, true_bools, pred_labels, model_output_object_val = validation(model, val_loader)\n",
    "\n",
    "    correct = len(true_bools)\n",
    "    total = len(true_labels)\n",
    "    #print(loss_val)\n",
    "    # Print accuracy\n",
    "    ave = sum(loss_val) / len(loss_val)\n",
    "    print('Accuracy for fold %d: %d %%' % (fold, 100.0 * ave))\n",
    "    print('--------------------------------')\n",
    "    results[fold] = 100.0 * ave\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# Print fold results\n",
    "print(f'K-FOLD CROSS VALIDATION RESULTS FOR {k_folds} FOLDS')\n",
    "print('--------------------------------')\n",
    "total = 0.0\n",
    "for key, value in results.items():\n",
    "    print(f'Fold {key}: {value} %')\n",
    "    total += value\n",
    "print(f'Average: {total/len(results.items())} %')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
